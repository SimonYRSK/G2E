import zarr
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import torch
from torch.utils.data import Dataset, DataLoader
import xarray as xr

# ===================== 1. å˜é‡æ˜ å°„è¡¨ =====================
gfs2era5_mapping = {
    "Temperature": {
        "era5_vars": ["t50", "t100", "t150", "t200", "t250", "t300", "t400", "t500", "t600", "t700", "t850", "t925", "t1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨æ°”æ¸© â†’ ERA5 50~1000hPaç­‰å‹é¢æ°”æ¸©ï¼ˆ13å±‚ï¼‰"
    },
    "2 metre temperature": {
        "era5_vars": ["t2m"],
        "var_type": "surface",
        "description": "GFS 2ç±³æ°”æ¸© â†’ ERA5 2ç±³æ°”æ¸©ï¼ˆt2mï¼‰"
    },
    "10 metre U wind component": {
        "era5_vars": ["u10m"],
        "var_type": "surface",
        "description": "GFS 10ç±³Ué£åˆ†é‡ â†’ ERA5 10ç±³çº¬å‘é£ï¼ˆu10mï¼‰"
    },
    "100 metre U wind component": {
        "era5_vars": ["u100m"],
        "var_type": "surface",
        "description": "GFS 100ç±³Ué£åˆ†é‡ â†’ ERA5 100ç±³çº¬å‘é£ï¼ˆu100mï¼‰"
    },
    "10 metre V wind component": {
        "era5_vars": ["v10m"],
        "var_type": "surface",
        "description": "GFS 10ç±³Vé£åˆ†é‡ â†’ ERA5 10ç±³ç»å‘é£ï¼ˆv10mï¼‰"
    },
    "100 metre V wind component": {
        "era5_vars": ["v100m"],
        "var_type": "surface",
        "description": "GFS 100ç±³Vé£åˆ†é‡ â†’ ERA5 100ç±³ç»å‘é£ï¼ˆv100mï¼‰"
    },
    "U component of wind": {
        "era5_vars": ["u50", "u100", "u150", "u200", "u250", "u300", "u400", "u500", "u600", "u700", "u850", "u925", "u1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨Ué£åˆ†é‡ â†’ ERA5 50~1000hPaç­‰å‹é¢çº¬å‘é£ï¼ˆ13å±‚ï¼‰"
    },
    "V component of wind": {
        "era5_vars": ["v50", "v100", "v150", "v200", "v250", "v300", "v400", "v500", "v600", "v700", "v850", "v925", "v1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨Vé£åˆ†é‡ â†’ ERA5 50~1000hPaç­‰å‹é¢ç»å‘é£ï¼ˆ13å±‚ï¼‰"
    },
    "Geopotential height": {
        "era5_vars": ["z50", "z100", "z150", "z200", "z250", "z300", "z400", "z500", "z600", "z700", "z850", "z925", "z1000"],
        "var_type": "upper_air",
        "description": "GFSä½åŠ¿é«˜åº¦ â†’ ERA5 50~1000hPaç­‰å‹é¢ä½åŠ¿é«˜åº¦ï¼ˆ13å±‚ï¼‰"
    },
    "2 metre dewpoint temperature": {
        "era5_vars": ["d2m"],
        "var_type": "surface",
        "description": "GFS 2ç±³éœ²ç‚¹æ¸©åº¦ â†’ ERA5 2ç±³éœ²ç‚¹æ¸©åº¦ï¼ˆd2mï¼‰"
    }
}

# åå‘æ˜ å°„ï¼šERA5å˜é‡å â†’ GFSé€šç”¨å˜é‡å
era52gfs_mapping = {}
for gfs_var, info in gfs2era5_mapping.items():
    for era5_var in info["era5_vars"]:
        era52gfs_mapping[era5_var] = gfs_var

# ===================== 2. å·¥å…·å‡½æ•°ï¼šç»Ÿä¸€å±‚æ•°å¡«å…… =====================
def pad_to_base_layers(data: np.ndarray, base_layers: int = 13, pad_mode: str = "repeat") -> np.ndarray:
    """
    å°†å˜é‡å¡«å……åˆ°åŸºå‡†å±‚æ•°ï¼ˆè§£å†³å•å±‚/13å±‚å˜é‡ç»´åº¦ä¸ä¸€è‡´é—®é¢˜ï¼‰
    Args:
        data: è¾“å…¥æ•°ç»„ï¼Œå½¢çŠ¶(D, H, W)ï¼ŒD=1æˆ–13
        base_layers: åŸºå‡†å±‚æ•°ï¼ˆé»˜è®¤13ï¼‰
        pad_mode: å¡«å……æ–¹å¼ - "repeat"ï¼ˆé‡å¤å•å±‚ï¼‰/"zero"ï¼ˆè¡¥é›¶ï¼‰
    Returns:
        å¡«å……åæ•°ç»„ï¼Œå½¢çŠ¶(base_layers, H, W)
    """
    D, H, W = data.shape
    if D == base_layers:
        return data
    
    if D != 1:
        raise ValueError(f"å˜é‡å±‚æ•°å¿…é¡»æ˜¯1æˆ–{base_layers}ï¼Œå½“å‰ä¸º{D}")
    
    if pad_mode == "repeat":
        # é‡å¤å¡«å……ï¼ˆç‰©ç†æ„ä¹‰æ›´åˆç†ï¼šåœ°é¢å˜é‡åœ¨æ‰€æœ‰é«˜åº¦å±‚å‡ä¸ºè¯¥å€¼ï¼‰
        return np.repeat(data, base_layers, axis=0)
    elif pad_mode == "zero":
        # è¡¥é›¶å¡«å……ï¼ˆä»…ç¬¬0å±‚æœ‰æ•ˆï¼‰
        pad_data = np.zeros((base_layers, H, W), dtype=data.dtype)
        pad_data[0:1, :, :] = data
        return pad_data
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å¡«å……æ–¹å¼ï¼š{pad_mode}ï¼Œå¯é€‰'repeat'/'zero'")

# ===================== 3. ERA5Reader =====================
class ERA5Reader:
    def __init__(self,
                zarr_path: str = "/cpfs01/projects-HDD/cfff-4a8d9af84f66_HDD/public/fanjiang/dataset/era5.2002_2024.c85.p25.h6",
                mapping: Dict = gfs2era5_mapping,
                start_dt: str = "2020-01-01 00:00:00",
                end_dt: str = "2024-12-31 18:00:00",
                reverse_mapping: Dict = era52gfs_mapping):
        
        # è·¯å¾„å¤„ç†ï¼ˆå…¼å®¹å­—ç¬¦ä¸²/Pathå¯¹è±¡ï¼‰
        self.era5_root = Path(zarr_path) if isinstance(zarr_path, str) else zarr_path
        self.mapping = mapping
        self.reverse_mapping = reverse_mapping
        
        # æ—¶é—´å‚æ•°åˆå§‹åŒ–
        self.start_dt = datetime.strptime(start_dt, "%Y-%m-%d %H:%M:%S")
        self.end_dt = datetime.strptime(end_dt, "%Y-%m-%d %H:%M:%S")
        self.base_time = datetime(2002, 1, 1, 0, 0, 0)  # ERA5æ•°æ®é›†åŸºå‡†æ—¶é—´
        self.time_step_hours = 6.0  # ERA5å›ºå®š6å°æ—¶æ—¶é—´æ­¥
        
        # é¢„ç¼“å­˜ï¼šGFSé€šç”¨å˜é‡å â†’ ERA5å˜é‡åˆ—è¡¨
        self.gfs2era5_vars = {k: v["era5_vars"] for k, v in mapping.items()}
        
        # æ‡’åŠ è½½Zarrå¥æŸ„å’Œæœ‰æ•ˆæ—¶é—´ç´¢å¼•
        self.data_zarr = None
        self.channel_zarr = None
        self.lat_zarr = None
        self.lon_zarr = None
        self.valid_time_indices = None  # ç­›é€‰åçš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•
        self.channel_name2idx = None    # channelåâ†’ç´¢å¼•æ˜ å°„
        self._load_zarr_handles()
        
        # é¢„ç”Ÿæˆæœ‰æ•ˆæ—¶é—´æˆ³åˆ—è¡¨ï¼ˆç´¢å¼•â†’å®é™…æ—¶é—´ï¼‰
        self.valid_timestamps = self._generate_valid_timestamps()

    def _load_zarr_handles(self):
        """æ‡’åŠ è½½Zarrå¥æŸ„ï¼ˆæ ¸å¿ƒï¼šé¿å…ä¸€æ¬¡æ€§åŠ è½½è¶…å¤§æ•°ç»„ï¼‰"""
        try:
            # åŠ è½½å„ç»´åº¦Zarrå¥æŸ„ï¼ˆåªè¯»æ¨¡å¼ï¼‰
            self.data_zarr = zarr.open(self.era5_root / "data", mode='r')
            self.channel_zarr = zarr.open(self.era5_root / "channel", mode='r')
            self.lat_zarr = zarr.open(self.era5_root / "lat", mode='r')
            self.lon_zarr = zarr.open(self.era5_root / "lon", mode='r')
            
            # åŸºç¡€ç»´åº¦ä¿¡æ¯
            self.time_steps = self.data_zarr.shape[0]  # timeç»´åº¦é•¿åº¦
            self.n_channels = self.data_zarr.shape[1]  # channelç»´åº¦é•¿åº¦
            self.lat_size = self.data_zarr.shape[2]    # çº¬åº¦æ•°
            self.lon_size = self.data_zarr.shape[3]    # ç»åº¦æ•°
            
            # æ ¡éªŒç©ºé—´ç»´åº¦ï¼ˆ721Ã—1440ï¼‰
            assert self.lat_size == 721 and self.lon_size == 1440, \
                f"ERA5ç©ºé—´ç»´åº¦é”™è¯¯ï¼šéœ€721Ã—1440ï¼Œå½“å‰{self.lat_size}Ã—{self.lon_size}"
            
            # å…¼å®¹å­—ç¬¦ä¸²/å­—èŠ‚ä¸²çš„channelå
            channel_raw = self.channel_zarr[:]
            self.channel_names = []
            for name in channel_raw:
                if isinstance(name, bytes):
                    self.channel_names.append(name.decode('utf-8'))
                elif isinstance(name, (str, np.str_)):
                    self.channel_names.append(str(name))
                else:
                    self.channel_names.append(str(name))
            
            # æ„å»ºchannelåâ†’ç´¢å¼•æ˜ å°„
            self.channel_name2idx = {name: idx for idx, name in enumerate(self.channel_names)}
            
            # ç­›é€‰æœ‰æ•ˆæ—¶é—´ç´¢å¼•
            self.valid_time_indices = self._time_filter()
            print(f"âœ… ERA5åŠ è½½å®Œæˆï¼š")
            print(f"   æœ‰æ•ˆæ—¶é—´æ­¥ï¼š{len(self.valid_time_indices)}ä¸ªï¼ˆ{self.start_dt}~{self.end_dt}ï¼‰")
            print(f"   æ€»é€šé“æ•°ï¼š{self.n_channels}ï¼Œç©ºé—´ç»´åº¦ï¼š{self.lat_size}Ã—{self.lon_size}")
            print(f"   å‰5ä¸ªé€šé“åï¼š{self.channel_names[:5]}")
        
        except Exception as e:
            raise RuntimeError(f"åŠ è½½ERA5 Zarrå¤±è´¥ï¼š{str(e)}")

    def _time_filter(self) -> List[int]:
        """ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•"""
        # è®¡ç®—èµ·å§‹/ç»“æŸæ—¶é—´å¯¹åº”çš„ç´¢å¼•
        delta_hours_start = (self.start_dt - self.base_time).total_seconds() / 3600
        start_idx = int(delta_hours_start // self.time_step_hours)
        
        delta_hours_end = (self.end_dt - self.base_time).total_seconds() / 3600
        end_idx = int(delta_hours_end // self.time_step_hours)
        
        # è¾¹ç•Œæ ¡éªŒ
        start_idx = max(0, start_idx)
        end_idx = min(self.time_steps - 1, end_idx)
        
        if start_idx > end_idx:
            raise ValueError(f"ERA5æ— æœ‰æ•ˆæ—¶é—´æ•°æ®ï¼š{self.start_dt}~{self.end_dt}ï¼ˆç´¢å¼•{start_idx}~{end_idx}è¶…å‡ºèŒƒå›´ï¼‰")
        
        return list(range(start_idx, end_idx + 1))

    def _generate_valid_timestamps(self) -> List[datetime]:
        """ç”Ÿæˆæœ‰æ•ˆæ—¶é—´ç´¢å¼•å¯¹åº”çš„å®é™…æ—¶é—´æˆ³åˆ—è¡¨"""
        valid_timestamps = []
        for time_idx in self.valid_time_indices:
            delta_hours = time_idx * self.time_step_hours
            timestamp = self.base_time + timedelta(hours=delta_hours)
            valid_timestamps.append(timestamp)
        return valid_timestamps

    def _get_nearest_time_idx(self, target_time: datetime) -> int:
        """æ ¹æ®ç›®æ ‡æ—¶é—´æ‰¾æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•"""
        delta_hours = (target_time - self.base_time).total_seconds() / 3600
        target_idx = int(delta_hours // self.time_step_hours)
        
        # æ‰¾æœ‰æ•ˆç´¢å¼•ä¸­æœ€æ¥è¿‘çš„
        valid_indices_arr = np.array(self.valid_time_indices)
        nearest_idx = valid_indices_arr[np.argmin(np.abs(valid_indices_arr - target_idx))]
        
        if nearest_idx not in self.valid_time_indices:
            raise ValueError(f"ç›®æ ‡æ—¶é—´{target_time}æ— å¯¹åº”çš„æœ‰æ•ˆERA5æ•°æ®")
        return nearest_idx

    def read_by_time(self, target_time: datetime, gfs_vars: Optional[List[str]] = None) -> Dict[str, np.ndarray]:
        """
        æŒ‰ç›®æ ‡æ—¶é—´è¯»å–ERA5æ•°æ®ï¼Œè¿”å›{GFSé€šç”¨å˜é‡å: (å±‚æ•°, lat, lon)}
        """
        gfs_vars = gfs_vars or list(self.gfs2era5_vars.keys())
        
        # 1. æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•
        time_idx = self._get_nearest_time_idx(target_time)
        
        # 2. é€ä¸ªè¯»å–GFSé€šç”¨å˜é‡å¯¹åº”çš„ERA5åˆ†å±‚å˜é‡
        result = {}
        for gfs_var in gfs_vars:
            if gfs_var not in self.gfs2era5_vars:
                raise ValueError(f"ERA5ä¸æ”¯æŒGFSé€šç”¨å˜é‡ï¼š{gfs_var}")
            
            era5_vars = self.gfs2era5_vars[gfs_var]
            layer_data_list = []
            
            for era5_var in era5_vars:
                if era5_var not in self.channel_name2idx:
                    raise ValueError(f"ERA5æ— è¯¥é€šé“ï¼š{era5_var}")
                chan_idx = self.channel_name2idx[era5_var]
                
                # è¯»å–æ•°æ®ï¼š(time, channel, lat, lon) â†’ å–æŒ‡å®štimeå’Œchannel
                var_data = self.data_zarr[time_idx, chan_idx, :, :]  # (lat, lon)
                layer_data_list.append(var_data)
            
            # æ‹¼æ¥åˆ†å±‚å˜é‡ â†’ (å±‚æ•°, lat, lon)
            var_data_3d = np.stack(layer_data_list, axis=0)
            result[gfs_var] = var_data_3d
            print(f"ğŸ“Œ ERA5é€šç”¨å˜é‡{gfs_var}ï¼šæ‹¼æ¥{len(era5_vars)}å±‚ï¼Œå½¢çŠ¶{var_data_3d.shape}")
        
        return result

    @property
    def time_index(self) -> pd.DatetimeIndex:
        """è¿”å›æœ‰æ•ˆæ—¶é—´æˆ³çš„DatetimeIndexï¼ˆå’ŒGFSReaderå¯¹é½ï¼‰"""
        return pd.DatetimeIndex(self.valid_timestamps)

    @property
    def all_gfs_vars(self) -> List[str]:
        """è¿”å›æ‰€æœ‰æ”¯æŒçš„GFSé€šç”¨å˜é‡å"""
        return list(self.gfs2era5_vars.keys())

# ===================== 4. GFSReader =====================
class GFSReader:
    def __init__(self,
                zarr_path: str = "/cpfs01/projects-HDD/cfff-4a8d9af84f66_HDD/public/database/gfs_2020_2024_c10",
                mapping: Dict = gfs2era5_mapping,
                start_dt: str = "2020-01-01 00:00:00",
                end_dt: str = "2024-12-31 18:00:00",
                reverse_mapping: Dict = era52gfs_mapping):
        
        # è·¯å¾„å¤„ç†
        self.gfs_root = Path(zarr_path) if isinstance(zarr_path, str) else zarr_path
        self.mapping = mapping
        self.reverse_mapping = reverse_mapping
        
        # æ—¶é—´å‚æ•°åˆå§‹åŒ–
        self.start_dt = datetime.strptime(start_dt, "%Y-%m-%d %H:%M:%S")
        self.end_dt = datetime.strptime(end_dt, "%Y-%m-%d %H:%M:%S")
        
        # é¢„ç¼“å­˜ï¼šGFSé€šç”¨å˜é‡å â†’ å˜é‡ç±»å‹ï¼ˆé«˜ç©º/åœ°é¢ï¼‰
        self.gfs_var_type = {k: v["var_type"] for k, v in mapping.items()}
        # é¢„ç¼“å­˜ï¼šæ‰€æœ‰æ”¯æŒçš„GFSå˜é‡å
        self.all_gfs_vars_list = list(mapping.keys())
        
        # æ‡’åŠ è½½å±æ€§
        self.ds = None  # xarray Datasetå¥æŸ„
        self.time_index = None  # æœ‰æ•ˆæ—¶é—´ç´¢å¼•ï¼ˆDatetimeIndexï¼‰
        self.valid_time_indices = None  # ç­›é€‰åçš„åŸå§‹æ—¶é—´ç´¢å¼•ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰
        self.valid_time_stamps = None   # ç­›é€‰åçš„æ—¶é—´æˆ³åˆ—è¡¨
        self.level_order = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]  # å›ºå®šå±‚çº§é¡ºåº
        
        # åˆå§‹åŒ–åŠ è½½
        self._load_zarr_handles()
        self._filter_valid_times()

    def _load_zarr_handles(self):
        """æ‡’åŠ è½½GFS Zarræ•°æ®ï¼ˆxarrayæ–¹å¼ï¼‰"""
        try:
            self.ds = xr.open_zarr(self.gfs_root)
            print(f"âœ… GFS ZarråŠ è½½å®Œæˆï¼šè·¯å¾„={self.gfs_root}")
            
            # éªŒè¯æ ¸å¿ƒç»´åº¦
            required_dims = ["time", "lat", "lon", "level"]
            missing_dims = [d for d in required_dims if d not in self.ds.dims]
            if missing_dims:
                raise ValueError(f"GFS Zarrç¼ºå¤±æ ¸å¿ƒç»´åº¦ï¼š{missing_dims}")
            
            # æ„å»ºå®Œæ•´çš„æ—¶é—´ç´¢å¼•
            self.full_time_index = pd.DatetimeIndex(self.ds["time"].values.astype('datetime64[s]'))
            print(f"   GFSæ€»æ—¶é—´èŒƒå›´ï¼š{self.full_time_index[0]} ~ {self.full_time_index[-1]}")
            print(f"   GFSæ€»æ—¶é—´æ­¥ï¼š{len(self.full_time_index)}")
            print(f"   GFSç©ºé—´ç»´åº¦ï¼šlat={self.ds.dims['lat']}, lon={self.ds.dims['lon']}")
        
        except Exception as e:
            raise RuntimeError(f"åŠ è½½GFS Zarrå¤±è´¥ï¼š{str(e)}")

    def _filter_valid_times(self):
        """ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œæ—¶é—´æˆ³"""
        mask = (self.full_time_index >= self.start_dt) & (self.full_time_index <= self.end_dt)
        self.valid_time_indices = np.where(mask)[0].tolist()
        self.valid_time_stamps = self.full_time_index[mask].tolist()
        
        if not self.valid_time_indices:
            raise ValueError(f"GFSæ— æœ‰æ•ˆæ—¶é—´æ•°æ®ï¼š{self.start_dt}~{self.end_dt}")
        
        self.time_index = pd.DatetimeIndex(self.valid_time_stamps)
        print(f"âœ… GFSæœ‰æ•ˆæ—¶é—´æ­¥ï¼š{len(self.valid_time_indices)}ä¸ªï¼ˆ{self.start_dt}~{self.end_dt}ï¼‰")

    def _get_nearest_time_idx(self, target_time: datetime) -> Tuple[int, datetime]:
        """æ‰¾åˆ°ç›®æ ‡æ—¶é—´æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œå¯¹åº”çš„æ—¶é—´æˆ³"""
        time_diffs = [abs((ts - target_time).total_seconds()) for ts in self.valid_time_stamps]
        nearest_pos = np.argmin(time_diffs)
        nearest_raw_idx = self.valid_time_indices[nearest_pos]
        nearest_ts = self.valid_time_stamps[nearest_pos]
        return nearest_raw_idx, nearest_ts

    def read_by_time(self, target_time: datetime, gfs_vars: Optional[List[str]] = None) -> Dict[str, np.ndarray]:
        """æŒ‰ç›®æ ‡æ—¶é—´è¯»å–GFSæ•°æ®ï¼ˆå’ŒERA5Readeræ¥å£å®Œå…¨å¯¹é½ï¼‰"""
        gfs_vars = gfs_vars or self.all_gfs_vars_list
        
        # 1. æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œæ—¶é—´æˆ³
        time_idx, target_time_str = self._get_nearest_time_idx(target_time)
        print(f"ğŸ“Œ è¯»å–GFSæ—¶é—´ï¼š{target_time_str}ï¼ˆç›®æ ‡æ—¶é—´ï¼š{target_time}ï¼‰")
        
        # 2. é€ä¸ªè¯»å–å˜é‡
        result = {}
        for gfs_var in gfs_vars:
            if gfs_var not in self.gfs_var_type:
                raise ValueError(f"GFSä¸æ”¯æŒè¯¥å˜é‡ï¼š{gfs_var}ï¼Œæ”¯æŒçš„å˜é‡ï¼š{self.all_gfs_vars_list}")
            
            var_type = self.gfs_var_type[gfs_var]
            var_data = self.ds[gfs_var].isel(time=time_idx)  # å–æŒ‡å®šæ—¶é—´
            
            if var_type == "upper_air":
                # é«˜ç©ºå˜é‡ï¼šæŒ‰å›ºå®šå±‚çº§é¡ºåºé‡æ–°æ’åº â†’ (13, 721, 1440)
                level_vals = self.ds["level"].values.tolist()
                try:
                    level_indices = [level_vals.index(level) for level in self.level_order]
                except ValueError as e:
                    raise ValueError(f"GFSå±‚çº§ç¼ºå¤±ï¼š{e}ï¼Œå½“å‰å±‚çº§ï¼š{level_vals}")
                var_data_sorted = var_data.isel(level=level_indices)
                var_arr = var_data_sorted.values  # (13, 721, 1440)
            else:
                # åœ°é¢å˜é‡ï¼šå¢åŠ å±‚æ•°ç»´åº¦ â†’ (1, 721, 1440)
                var_arr = var_data.values[np.newaxis, :, :]
            
            # å¤„ç†ç¼ºå¤±å€¼
            var_arr = np.nan_to_num(var_arr, nan=0.0)
            result[gfs_var] = var_arr
            print(f"   GFSå˜é‡{gfs_var}ï¼šå½¢çŠ¶{var_arr.shape}ï¼ˆ{var_type}ï¼‰")
        
        return result

    @property
    def all_gfs_vars(self) -> List[str]:
        """è¿”å›æ‰€æœ‰æ”¯æŒçš„GFSå˜é‡å"""
        return self.all_gfs_vars_list

    def close(self):
        """å…³é—­Datasetå¥æŸ„ï¼Œé‡Šæ”¾èµ„æº"""
        if self.ds is not None:
            self.ds.close()
            print("âœ… GFS Datasetå·²å…³é—­")

# ===================== 5. æ ¸å¿ƒDatasetï¼šGFSERA5PairDataset =====================
class GFSERA5PairDataset(Dataset):
    def __init__(
        self,
        gfs_reader: GFSReader,
        era5_reader: ERA5Reader,
        gfs_vars: Optional[List[str]] = None,
        normalize: bool = False,
        base_layers: int = 13,
        pad_mode: str = "repeat"
    ):
        self.gfs_reader = gfs_reader
        self.era5_reader = era5_reader
        self.gfs_vars = gfs_vars or gfs_reader.all_gfs_vars
        self.normalize = normalize
        self.base_layers = base_layers  # ç»Ÿä¸€å±‚æ•°çš„åŸºå‡†
        self.pad_mode = pad_mode        # å¡«å……æ–¹å¼
        
        # æ—¶é—´æˆ³å¯¹é½
        gfs_time_set = set(gfs_reader.time_index)
        era5_time_set = set(era5_reader.time_index)
        self.common_timestamps = sorted(list(gfs_time_set & era5_time_set))
        
        if len(self.common_timestamps) == 0:
            raise ValueError("GFSå’ŒERA5æ— é‡å çš„æœ‰æ•ˆæ—¶é—´æˆ³ï¼")
        
        print(f"âœ… æ—¶é—´æˆ³å¯¹é½å®Œæˆï¼šå…±æ‰¾åˆ° {len(self.common_timestamps)} ä¸ªé‡å æ—¶é—´æˆ³")
        
        # é¢„è®¡ç®—å½’ä¸€åŒ–å‚æ•°
        if self.normalize:
            self.norm_params = self._compute_normalize_params()

    def _compute_normalize_params(self) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
        """æŒ‰å˜é‡ç‹¬ç«‹è®¡ç®—å½’ä¸€åŒ–å‚æ•°ï¼ˆå…ˆå¡«å……åˆ°åŸºå‡†å±‚æ•°ï¼‰"""
        norm_params = {}
        sample_num = min(100, len(self.common_timestamps))
        
        for var_name in self.gfs_vars:
            sample_data = []
            for ts in self.common_timestamps[:sample_num]:
                gfs_data = self.gfs_reader.read_by_time(ts, [var_name])[var_name]
                era5_data = self.era5_reader.read_by_time(ts, [var_name])[var_name]
                
                # å…ˆå¡«å……åˆ°åŸºå‡†å±‚æ•°
                gfs_data = pad_to_base_layers(gfs_data, self.base_layers, self.pad_mode)
                era5_data = pad_to_base_layers(era5_data, self.base_layers, self.pad_mode)
                
                sample_data.append(gfs_data)
                sample_data.append(era5_data)
            
            sample_stack = np.stack(sample_data)
            # æŒ‰å±‚æ•°ç»´åº¦è®¡ç®—å‡å€¼/æ ‡å‡†å·®ï¼ˆä¿æŒç»´åº¦ï¼š(base_layers,1,1)ï¼‰
            mean = np.mean(sample_stack, axis=(0, 2, 3), keepdims=True)
            std = np.std(sample_stack, axis=(0, 2, 3), keepdims=True) + 1e-8
            norm_params[var_name] = (mean, std)
        
        print(f"âœ… å½’ä¸€åŒ–å‚æ•°è®¡ç®—å®Œæˆï¼ˆå˜é‡æ•°ï¼š{len(norm_params)}ï¼‰")
        return norm_params

    def _normalize_data(self, data: np.ndarray, var_name: str) -> np.ndarray:
        """å¯¹å¡«å……åçš„å˜é‡æ•°æ®åšå½’ä¸€åŒ–"""
        mean, std = self.norm_params[var_name]
        return (data - mean) / std

    def __len__(self) -> int:
        return len(self.common_timestamps)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:
        """
        ä¿®å¤åè¿”å›ï¼š
            gfs_tensor: (base_layers, V, 721, 1440)  4ç»´ï¼šå±‚æ•°â†’å˜é‡â†’çº¬åº¦â†’ç»åº¦
            era5_tensor: (base_layers, V, 721, 1440)
            timestamp_str: æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        timestamp = self.common_timestamps[idx]
        timestamp_str = timestamp.strftime("%Y-%m-%d %H:%M:%S")
        
        # å­˜å‚¨æ¯ä¸ªå˜é‡å¤„ç†åçš„å¼ é‡ï¼š(base_layers, 1, 721, 1440)
        gfs_var_tensors = []
        era5_var_tensors = []
        
        for var_name in self.gfs_vars:
            # 1. è¯»å–åŸå§‹æ•°æ®ï¼ˆD, 721, 1440ï¼‰
            gfs_data = self.gfs_reader.read_by_time(timestamp, [var_name])[var_name]  # (D,721,1440)
            era5_data = self.era5_reader.read_by_time(timestamp, [var_name])[var_name]  # (D,721,1440)
            
            # 2. ç»Ÿä¸€å¡«å……åˆ°åŸºå‡†å±‚æ•°ï¼ˆæ ¸å¿ƒï¼šè§£å†³å•å±‚/13å±‚ä¸ä¸€è‡´ï¼‰
            gfs_data_padded = pad_to_base_layers(gfs_data, self.base_layers, self.pad_mode)  # (13,721,1440)
            era5_data_padded = pad_to_base_layers(era5_data, self.base_layers, self.pad_mode)  # (13,721,1440)
            
            # 3. å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
            if self.normalize:
                gfs_data_padded = self._normalize_data(gfs_data_padded, var_name)
                era5_data_padded = self._normalize_data(era5_data_padded, var_name)
            
            # 4. è½¬æ¢ä¸ºTensorå¹¶æ·»åŠ å˜é‡ç»´åº¦ â†’ (13, 1, 721, 1440)
            #    å…³é”®ï¼šunsqueeze(1) æ˜¯åœ¨ç¬¬1ç»´ï¼ˆå˜é‡ç»´ï¼‰æ·»åŠ ç»´åº¦ï¼Œä¸æ˜¯ç¬¬0ç»´ï¼
            gfs_tensor = torch.from_numpy(gfs_data_padded).float().unsqueeze(1)
            era5_tensor = torch.from_numpy(era5_data_padded).float().unsqueeze(1)
            
            gfs_var_tensors.append(gfs_tensor)
            era5_var_tensors.append(era5_tensor)
        
        # 5. åˆå¹¶æ‰€æœ‰å˜é‡åˆ°ç¬¬1ç»´ï¼ˆå˜é‡ç»´ï¼‰ â†’ (13, V, 721, 1440)
        #    å…³é”®ï¼šdim=1 æ˜¯åˆå¹¶å˜é‡ç»´åº¦ï¼Œä¸æ˜¯dim=0ï¼
        gfs_combined = torch.cat(gfs_var_tensors, dim=1)
        era5_combined = torch.cat(era5_var_tensors, dim=1)
        
        # è°ƒè¯•ï¼šæ‰“å°å•æ ·æœ¬ç»´åº¦ï¼ˆç¡®è®¤æ­£ç¡®ï¼‰
        if idx == 0:
            print(f"\n=== å•æ ·æœ¬ç»´åº¦éªŒè¯ ===")
            print(f"å•æ ·æœ¬å½¢çŠ¶ï¼š{gfs_combined.shape}")
            print(f"  - å±‚æ•°ç»´åº¦ï¼š{gfs_combined.shape[0]}")
            print(f"  - å˜é‡ç»´åº¦ï¼š{gfs_combined.shape[1]}")
            print(f"  - ç©ºé—´ç»´åº¦ï¼š{gfs_combined.shape[2]}Ã—{gfs_combined.shape[3]}")
        
        return gfs_combined, era5_combined, timestamp_str


# ===================== 6. è‡ªå®šä¹‰Collateå‡½æ•°ï¼šèåˆBatch+å±‚æ•° =====================
def collate_fn(batch, base_layers: int = 13):
    """
    å¼ºåˆ¶ç¡®ä¿å±‚æ•°èå…¥Batchç»´åº¦ï¼Œè¾“å‡º4ç»´å¼ é‡
    """
    gfs_list = []
    era5_list = []
    ts_batch = []
    
    for gfs_tensor, era5_tensor, ts in batch:
        # éªŒè¯å•æ ·æœ¬å½¢çŠ¶ï¼šå¿…é¡»æ˜¯ (13, V, 721, 1440)
        assert len(gfs_tensor.shape) == 4 and gfs_tensor.shape[0] == base_layers, \
            f"å•æ ·æœ¬å½¢çŠ¶é”™è¯¯ï¼Œéœ€({base_layers}, V, 721, 1440)ï¼Œå½“å‰{gfs_tensor.shape}"
        
        gfs_list.append(gfs_tensor)
        era5_list.append(era5_tensor)
        ts_batch.append(ts)
    
    # 1. å †å batchç»´åº¦ â†’ (B, 13, V, 721, 1440)
    gfs_stack = torch.stack(gfs_list, dim=0)
    era5_stack = torch.stack(era5_list, dim=0)
    
    # 2. å¼ºåˆ¶èåˆBå’Œå±‚æ•°ç»´åº¦ â†’ (BÃ—13, V, 721, 1440)
    #    - size()è·å–ç»´åº¦å€¼ï¼Œreshapeå¼ºåˆ¶é‡æ„
    B = gfs_stack.shape[0]
    V = gfs_stack.shape[2]
    H = gfs_stack.shape[3]
    W = gfs_stack.shape[4]
    
    gfs_batch = gfs_stack.reshape(B * base_layers, V, H, W)
    era5_batch = era5_stack.reshape(B * base_layers, V, H, W)
    
    # æ‰“å°æ­£ç¡®çš„ç»´åº¦æ—¥å¿—
    print(f"=== æ‰¹é‡ç»´åº¦ ===")
    print(f"Batchç»´åº¦ï¼ˆBÃ—13ï¼‰ï¼š{gfs_batch.shape[0]}ï¼ˆ{B}Ã—{base_layers}={B*base_layers}ï¼‰")
    print(f"Channelsç»´åº¦ï¼ˆå˜é‡æ•°ï¼‰ï¼š{gfs_batch.shape[1]}ï¼ˆ{V}ä¸ªï¼‰")
    print(f"æœ€ç»ˆå¼ é‡å½¢çŠ¶ï¼š{gfs_batch.shape}")
    print(f"æ‰¹é‡æ—¶é—´æˆ³ï¼š{ts_batch}")
    
    return gfs_batch, era5_batch, ts_batch
# ===================== 7. æµ‹è¯•ä¸»å‡½æ•° =====================
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–Reader
    gfs_reader = GFSReader(
        start_dt="2020-01-01 00:00:00",
        end_dt="2020-01-02 18:00:00"
    )
    era5_reader = ERA5Reader(
        start_dt="2020-01-01 00:00:00",
        end_dt="2020-01-02 18:00:00"
    )
    
    # 2. é€‰æ‹©è¦è®­ç»ƒçš„å˜é‡ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
    train_vars = [
        "Temperature",              # 13å±‚
        "10 metre U wind component" # 1å±‚
    ]
    
    # 3. åˆå§‹åŒ–æ•°æ®é›†
    dataset = GFSERA5PairDataset(
        gfs_reader=gfs_reader,
        era5_reader=era5_reader,
        gfs_vars=train_vars,
        normalize=True,
        base_layers=13,
        pad_mode="repeat"  # æ¨èç”¨repeatï¼ˆç‰©ç†æ„ä¹‰æ›´åˆç†ï¼‰
    )
    
    # 4. æµ‹è¯•å•æ ·æœ¬
    gfs_tensor, era5_tensor, ts_str = dataset[0]
    print(f"\n=== å•æ ·æœ¬ç»´åº¦ ===")
    print(f"æ—¶é—´æˆ³ï¼š{ts_str}")
    print(f"å•æ ·æœ¬å½¢çŠ¶ï¼ˆå±‚æ•°Ã—å˜é‡æ•°Ã—çº¬åº¦Ã—ç»åº¦ï¼‰ï¼š{gfs_tensor.shape}")
    print(f"  - å±‚æ•°ç»´åº¦ï¼š{gfs_tensor.shape[0]}ï¼ˆç»Ÿä¸€ä¸º13å±‚ï¼‰")
    print(f"  - å˜é‡æ•°ï¼ˆChannelsï¼‰ï¼š{gfs_tensor.shape[1]}ï¼ˆ{len(train_vars)}ä¸ªï¼‰")
    print(f"  - ç©ºé—´ç»´åº¦ï¼š{gfs_tensor.shape[2]}Ã—{gfs_tensor.shape[3]}")
    
    # 5. åˆå§‹åŒ–DataLoaderï¼ˆbatch_size=2ï¼‰
    batch_size = 2
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        # å¼ºåˆ¶æŒ‡å®šcollate_fnï¼Œé¿å…ä½¿ç”¨é»˜è®¤å‡½æ•°
        collate_fn=lambda x: collate_fn(x, base_layers=13)
    )
    
    # 6. æµ‹è¯•æ‰¹é‡æ•°æ®
    # æ›¿æ¢åŸæœ‰æµ‹è¯•å¾ªç¯
    for batch_idx, (gfs_batch, era5_batch, ts_batch) in enumerate(dataloader):
        # è¿™é‡Œæ— éœ€é¢å¤–æ‰“å°ï¼Œcollate_fnå†…éƒ¨å·²æ‰“å°æ­£ç¡®æ—¥å¿—
        if batch_idx >= 1:
            break
    
    # 7. å…³é—­èµ„æº
    gfs_reader.close()
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")