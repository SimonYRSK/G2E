import zarr
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import torch
from torch.utils.data import Dataset, DataLoader
import xarray as xr


gfs2era5_mapping = {
    "Temperature": {
        "era5_vars": ["t50", "t100", "t150", "t200", "t250", "t300", "t400", "t500", "t600", "t700", "t850", "t925", "t1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨æ°”æ¸© â†’ ERA5 50~1000hPaç­‰å‹é¢æ°”æ¸©ï¼ˆ13å±‚ï¼‰"
    },
    "2 metre temperature": {
        "era5_vars": ["t2m"],
        "var_type": "surface",
        "description": "GFS 2ç±³æ°”æ¸© â†’ ERA5 2ç±³æ°”æ¸©ï¼ˆt2mï¼‰"
    },
    "10 metre U wind component": {
        "era5_vars": ["u10m"],
        "var_type": "surface",
        "description": "GFS 10ç±³Ué£åˆ†é‡ â†’ ERA5 10ç±³çº¬å‘é£ï¼ˆu10mï¼‰"
    },
    "100 metre U wind component": {
        "era5_vars": ["u100m"],
        "var_type": "surface",
        "description": "GFS 100ç±³Ué£åˆ†é‡ â†’ ERA5 100ç±³çº¬å‘é£ï¼ˆu100mï¼‰"
    },
    "10 metre V wind component": {
        "era5_vars": ["v10m"],
        "var_type": "surface",
        "description": "GFS 10ç±³Vé£åˆ†é‡ â†’ ERA5 10ç±³ç»å‘é£ï¼ˆv10mï¼‰"
    },
    "100 metre V wind component": {
        "era5_vars": ["v100m"],
        "var_type": "surface",
        "description": "GFS 100ç±³Vé£åˆ†é‡ â†’ ERA5 100ç±³ç»å‘é£ï¼ˆv100mï¼‰"
    },
    "U component of wind": {
        "era5_vars": ["u50", "u100", "u150", "u200", "u250", "u300", "u400", "u500", "u600", "u700", "u850", "u925", "u1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨Ué£åˆ†é‡ â†’ ERA5 50~1000hPaç­‰å‹é¢çº¬å‘é£ï¼ˆ13å±‚ï¼‰"
    },
    "V component of wind": {
        "era5_vars": ["v50", "v100", "v150", "v200", "v250", "v300", "v400", "v500", "v600", "v700", "v850", "v925", "v1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨Vé£åˆ†é‡ â†’ ERA5 50~1000hPaç­‰å‹é¢ç»å‘é£ï¼ˆ13å±‚ï¼‰"
    },
    "Geopotential height": {
        "era5_vars": ["z50", "z100", "z150", "z200", "z250", "z300", "z400", "z500", "z600", "z700", "z850", "z925", "z1000"],
        "var_type": "upper_air",
        "description": "GFSä½åŠ¿é«˜åº¦ â†’ ERA5 50~1000hPaç­‰å‹é¢ä½åŠ¿é«˜åº¦ï¼ˆ13å±‚ï¼‰"
    },
    "2 metre dewpoint temperature": {
        "era5_vars": ["d2m"],
        "var_type": "surface",
        "description": "GFS 2ç±³éœ²ç‚¹æ¸©åº¦ â†’ ERA5 2ç±³éœ²ç‚¹æ¸©åº¦ï¼ˆd2mï¼‰"
    }
}

# åå‘æ˜ å°„ï¼šERA5å˜é‡å â†’ GFSé€šç”¨å˜é‡å
era52gfs_mapping = {}
for gfs_var, info in gfs2era5_mapping.items():
    for era5_var in info["era5_vars"]:
        era52gfs_mapping[era5_var] = gfs_var


class ERA5Reader:
    def __init__(self,
                zarr_path: str = "/cpfs01/projects-HDD/cfff-4a8d9af84f66_HDD/public/fanjiang/dataset/era5.2002_2024.c85.p25.h6",
                mapping: Dict = gfs2era5_mapping,
                start_dt: str = "2020-01-01 00:00:00",
                end_dt: str = "2024-12-31 18:00:00",
                reverse_mapping: Dict = era52gfs_mapping):
        
        # è·¯å¾„å¤„ç†ï¼ˆå…¼å®¹å­—ç¬¦ä¸²/Pathå¯¹è±¡ï¼‰
        self.era5_root = Path(zarr_path) if isinstance(zarr_path, str) else zarr_path
        self.mapping = mapping
        self.reverse_mapping = reverse_mapping
        
        # æ—¶é—´å‚æ•°åˆå§‹åŒ–
        self.start_dt = datetime.strptime(start_dt, "%Y-%m-%d %H:%M:%S")
        self.end_dt = datetime.strptime(end_dt, "%Y-%m-%d %H:%M:%S")
        self.base_time = datetime(2002, 1, 1, 0, 0, 0)  # ERA5æ•°æ®é›†åŸºå‡†æ—¶é—´
        self.time_step_hours = 6.0  # ERA5å›ºå®š6å°æ—¶æ—¶é—´æ­¥
        
        # é¢„ç¼“å­˜ï¼šGFSé€šç”¨å˜é‡å â†’ ERA5å˜é‡åˆ—è¡¨
        self.gfs2era5_vars = {k: v["era5_vars"] for k, v in mapping.items()}
        
        # æ‡’åŠ è½½Zarrå¥æŸ„å’Œæœ‰æ•ˆæ—¶é—´ç´¢å¼•
        self.data_zarr = None
        self.channel_zarr = None
        self.lat_zarr = None
        self.lon_zarr = None
        self.valid_time_indices = None  # ç­›é€‰åçš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•
        self.channel_name2idx = None    # channelåâ†’ç´¢å¼•æ˜ å°„
        self._load_zarr_handles()
        
        # é¢„ç”Ÿæˆæœ‰æ•ˆæ—¶é—´æˆ³åˆ—è¡¨ï¼ˆç´¢å¼•â†’å®é™…æ—¶é—´ï¼‰
        self.valid_timestamps = self._generate_valid_timestamps()

    def _load_zarr_handles(self):
        """æ‡’åŠ è½½Zarrå¥æŸ„ï¼ˆæ ¸å¿ƒï¼šé¿å…ä¸€æ¬¡æ€§åŠ è½½è¶…å¤§æ•°ç»„ï¼‰"""
        try:
            # åŠ è½½å„ç»´åº¦Zarrå¥æŸ„ï¼ˆåªè¯»æ¨¡å¼ï¼‰
            self.data_zarr = zarr.open(self.era5_root / "data", mode='r')
            self.channel_zarr = zarr.open(self.era5_root / "channel", mode='r')
            self.lat_zarr = zarr.open(self.era5_root / "lat", mode='r')
            self.lon_zarr = zarr.open(self.era5_root / "lon", mode='r')
            
            # åŸºç¡€ç»´åº¦ä¿¡æ¯
            self.time_steps = self.data_zarr.shape[0]  # timeç»´åº¦é•¿åº¦
            self.n_channels = self.data_zarr.shape[1]  # channelç»´åº¦é•¿åº¦
            self.lat_size = self.data_zarr.shape[2]    # çº¬åº¦æ•°
            self.lon_size = self.data_zarr.shape[3]    # ç»åº¦æ•°
            
            # æ ¡éªŒç©ºé—´ç»´åº¦ï¼ˆ721Ã—1440ï¼‰
            assert self.lat_size == 721 and self.lon_size == 1440, \
                f"ERA5ç©ºé—´ç»´åº¦é”™è¯¯ï¼šéœ€721Ã—1440ï¼Œå½“å‰{self.lat_size}Ã—{self.lon_size}"
            
            # å…¼å®¹å­—ç¬¦ä¸²/å­—èŠ‚ä¸²çš„channelå
            channel_raw = self.channel_zarr[:]
            self.channel_names = []
            for name in channel_raw:
                if isinstance(name, bytes):
                    self.channel_names.append(name.decode('utf-8'))
                elif isinstance(name, (str, np.str_)):
                    self.channel_names.append(str(name))
                else:
                    self.channel_names.append(str(name))
            
            # æ„å»ºchannelåâ†’ç´¢å¼•æ˜ å°„
            self.channel_name2idx = {name: idx for idx, name in enumerate(self.channel_names)}
            
            # ç­›é€‰æœ‰æ•ˆæ—¶é—´ç´¢å¼•
            self.valid_time_indices = self._time_filter()
            print(f"âœ… ERA5åŠ è½½å®Œæˆï¼š")
            print(f"   æœ‰æ•ˆæ—¶é—´æ­¥ï¼š{len(self.valid_time_indices)}ä¸ªï¼ˆ{self.start_dt}~{self.end_dt}ï¼‰")
            print(f"   æ€»é€šé“æ•°ï¼š{self.n_channels}ï¼Œç©ºé—´ç»´åº¦ï¼š{self.lat_size}Ã—{self.lon_size}")
            print(f"   å‰5ä¸ªé€šé“åï¼š{self.channel_names[:5]}")
        
        except Exception as e:
            raise RuntimeError(f"åŠ è½½ERA5 Zarrå¤±è´¥ï¼š{str(e)}")

    def _time_filter(self) -> List[int]:
        """ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•"""
        # è®¡ç®—èµ·å§‹/ç»“æŸæ—¶é—´å¯¹åº”çš„ç´¢å¼•
        delta_hours_start = (self.start_dt - self.base_time).total_seconds() / 3600
        start_idx = int(delta_hours_start // self.time_step_hours)
        
        delta_hours_end = (self.end_dt - self.base_time).total_seconds() / 3600
        end_idx = int(delta_hours_end // self.time_step_hours)
        
        # è¾¹ç•Œæ ¡éªŒ
        start_idx = max(0, start_idx)
        end_idx = min(self.time_steps - 1, end_idx)
        
        if start_idx > end_idx:
            raise ValueError(f"ERA5æ— æœ‰æ•ˆæ—¶é—´æ•°æ®ï¼š{self.start_dt}~{self.end_dt}ï¼ˆç´¢å¼•{start_idx}~{end_idx}è¶…å‡ºèŒƒå›´ï¼‰")
        
        return list(range(start_idx, end_idx + 1))

    def _generate_valid_timestamps(self) -> List[datetime]:
        """ç”Ÿæˆæœ‰æ•ˆæ—¶é—´ç´¢å¼•å¯¹åº”çš„å®é™…æ—¶é—´æˆ³åˆ—è¡¨"""
        valid_timestamps = []
        for time_idx in self.valid_time_indices:
            delta_hours = time_idx * self.time_step_hours
            timestamp = self.base_time + timedelta(hours=delta_hours)
            valid_timestamps.append(timestamp)
        return valid_timestamps

    def _get_nearest_time_idx(self, target_time: datetime) -> int:
        """æ ¹æ®ç›®æ ‡æ—¶é—´æ‰¾æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•"""
        delta_hours = (target_time - self.base_time).total_seconds() / 3600
        target_idx = int(delta_hours // self.time_step_hours)
        
        # æ‰¾æœ‰æ•ˆç´¢å¼•ä¸­æœ€æ¥è¿‘çš„
        valid_indices_arr = np.array(self.valid_time_indices)
        nearest_idx = valid_indices_arr[np.argmin(np.abs(valid_indices_arr - target_idx))]
        
        if nearest_idx not in self.valid_time_indices:
            raise ValueError(f"ç›®æ ‡æ—¶é—´{target_time}æ— å¯¹åº”çš„æœ‰æ•ˆERA5æ•°æ®")
        return nearest_idx

    def read_by_time(self, target_time: datetime, gfs_vars: Optional[List[str]] = None) -> Dict[str, np.ndarray]:
        """
        æŒ‰ç›®æ ‡æ—¶é—´è¯»å–ERA5æ•°æ®ï¼Œè¿”å›{GFSé€šç”¨å˜é‡å: (å±‚æ•°, lat, lon)}
        """
        gfs_vars = gfs_vars or list(self.gfs2era5_vars.keys())
        
        # 1. æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•
        time_idx = self._get_nearest_time_idx(target_time)
        
        # 2. é€ä¸ªè¯»å–GFSé€šç”¨å˜é‡å¯¹åº”çš„ERA5åˆ†å±‚å˜é‡
        result = {}
        for gfs_var in gfs_vars:
            if gfs_var not in self.gfs2era5_vars:
                raise ValueError(f"ERA5ä¸æ”¯æŒGFSé€šç”¨å˜é‡ï¼š{gfs_var}")
            
            era5_vars = self.gfs2era5_vars[gfs_var]
            layer_data_list = []
            
            for era5_var in era5_vars:
                if era5_var not in self.channel_name2idx:
                    raise ValueError(f"ERA5æ— è¯¥é€šé“ï¼š{era5_var}")
                chan_idx = self.channel_name2idx[era5_var]
                
                # è¯»å–æ•°æ®ï¼š(time, channel, lat, lon) â†’ å–æŒ‡å®štimeå’Œchannel
                var_data = self.data_zarr[time_idx, chan_idx, :, :]  # (lat, lon)
                layer_data_list.append(var_data)
            
            # æ‹¼æ¥åˆ†å±‚å˜é‡ â†’ (å±‚æ•°, lat, lon)
            var_data_3d = np.stack(layer_data_list, axis=0)
            result[gfs_var] = var_data_3d
            print(f"ğŸ“Œ ERA5é€šç”¨å˜é‡{gfs_var}ï¼šæ‹¼æ¥{len(era5_vars)}å±‚ï¼Œå½¢çŠ¶{var_data_3d.shape}")
        
        return result

    @property
    def time_index(self) -> pd.DatetimeIndex:
        """è¿”å›æœ‰æ•ˆæ—¶é—´æˆ³çš„DatetimeIndexï¼ˆå’ŒGFSReaderå¯¹é½ï¼‰"""
        return pd.DatetimeIndex(self.valid_timestamps)

    @property
    def all_gfs_vars(self) -> List[str]:
        """è¿”å›æ‰€æœ‰æ”¯æŒçš„GFSé€šç”¨å˜é‡å"""
        return list(self.gfs2era5_vars.keys())


class GFSReader:
    def __init__(self,
                zarr_path: str = "/cpfs01/projects-HDD/cfff-4a8d9af84f66_HDD/public/database/gfs_2020_2024_c10",
                mapping: Dict = gfs2era5_mapping,
                start_dt: str = "2020-01-01 00:00:00",
                end_dt: str = "2024-12-31 18:00:00",
                reverse_mapping: Dict = era52gfs_mapping):
        
        # è·¯å¾„å¤„ç†
        self.gfs_root = Path(zarr_path) if isinstance(zarr_path, str) else zarr_path
        self.mapping = mapping
        self.reverse_mapping = reverse_mapping
        
        # æ—¶é—´å‚æ•°åˆå§‹åŒ–
        self.start_dt = datetime.strptime(start_dt, "%Y-%m-%d %H:%M:%S")
        self.end_dt = datetime.strptime(end_dt, "%Y-%m-%d %H:%M:%S")
        
        # é¢„ç¼“å­˜ï¼šGFSé€šç”¨å˜é‡å â†’ å˜é‡ç±»å‹ï¼ˆé«˜ç©º/åœ°é¢ï¼‰
        self.gfs_var_type = {k: v["var_type"] for k, v in mapping.items()}
        # é¢„ç¼“å­˜ï¼šæ‰€æœ‰æ”¯æŒçš„GFSå˜é‡å
        self.all_gfs_vars_list = list(mapping.keys())
        
        # æ‡’åŠ è½½å±æ€§
        self.ds = None  # xarray Datasetå¥æŸ„
        self.time_index = None  # æœ‰æ•ˆæ—¶é—´ç´¢å¼•ï¼ˆDatetimeIndexï¼‰
        self.valid_time_indices = None  # ç­›é€‰åçš„åŸå§‹æ—¶é—´ç´¢å¼•ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰
        self.valid_time_stamps = None   # ç­›é€‰åçš„æ—¶é—´æˆ³åˆ—è¡¨
        self.level_order = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]  # å›ºå®šå±‚çº§é¡ºåº
        
        # åˆå§‹åŒ–åŠ è½½
        self._load_zarr_handles()
        self._filter_valid_times()

    def _load_zarr_handles(self):
        """æ‡’åŠ è½½GFS Zarræ•°æ®ï¼ˆxarrayæ–¹å¼ï¼‰"""
        try:
            self.ds = xr.open_zarr(self.gfs_root)
            print(f"âœ… GFS ZarråŠ è½½å®Œæˆï¼šè·¯å¾„={self.gfs_root}")
            
            # éªŒè¯æ ¸å¿ƒç»´åº¦
            required_dims = ["time", "lat", "lon", "level"]
            missing_dims = [d for d in required_dims if d not in self.ds.dims]
            if missing_dims:
                raise ValueError(f"GFS Zarrç¼ºå¤±æ ¸å¿ƒç»´åº¦ï¼š{missing_dims}")
            
            # æ„å»ºå®Œæ•´çš„æ—¶é—´ç´¢å¼•
            self.full_time_index = pd.DatetimeIndex(self.ds["time"].values.astype('datetime64[s]'))
            print(f"   GFSæ€»æ—¶é—´èŒƒå›´ï¼š{self.full_time_index[0]} ~ {self.full_time_index[-1]}")
            print(f"   GFSæ€»æ—¶é—´æ­¥ï¼š{len(self.full_time_index)}")
            print(f"   GFSç©ºé—´ç»´åº¦ï¼šlat={self.ds.dims['lat']}, lon={self.ds.dims['lon']}")
        
        except Exception as e:
            raise RuntimeError(f"åŠ è½½GFS Zarrå¤±è´¥ï¼š{str(e)}")

    def _filter_valid_times(self):
        """ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œæ—¶é—´æˆ³"""
        mask = (self.full_time_index >= self.start_dt) & (self.full_time_index <= self.end_dt)
        self.valid_time_indices = np.where(mask)[0].tolist()
        self.valid_time_stamps = self.full_time_index[mask].tolist()
        
        if not self.valid_time_indices:
            raise ValueError(f"GFSæ— æœ‰æ•ˆæ—¶é—´æ•°æ®ï¼š{self.start_dt}~{self.end_dt}")
        
        self.time_index = pd.DatetimeIndex(self.valid_time_stamps)
        print(f"âœ… GFSæœ‰æ•ˆæ—¶é—´æ­¥ï¼š{len(self.valid_time_indices)}ä¸ªï¼ˆ{self.start_dt}~{self.end_dt}ï¼‰")

    def _get_nearest_time_idx(self, target_time: datetime) -> Tuple[int, datetime]:
        """æ‰¾åˆ°ç›®æ ‡æ—¶é—´æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œå¯¹åº”çš„æ—¶é—´æˆ³"""
        time_diffs = [abs((ts - target_time).total_seconds()) for ts in self.valid_time_stamps]
        nearest_pos = np.argmin(time_diffs)
        nearest_raw_idx = self.valid_time_indices[nearest_pos]
        nearest_ts = self.valid_time_stamps[nearest_pos]
        return nearest_raw_idx, nearest_ts

    def read_by_time(self, target_time: datetime, gfs_vars: Optional[List[str]] = None) -> Dict[str, np.ndarray]:
        """æŒ‰ç›®æ ‡æ—¶é—´è¯»å–GFSæ•°æ®ï¼ˆå’ŒERA5Readeræ¥å£å®Œå…¨å¯¹é½ï¼‰"""
        gfs_vars = gfs_vars or self.all_gfs_vars_list
        
        # 1. æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œæ—¶é—´æˆ³
        time_idx, target_time_str = self._get_nearest_time_idx(target_time)
        print(f"ğŸ“Œ è¯»å–GFSæ—¶é—´ï¼š{target_time_str}ï¼ˆç›®æ ‡æ—¶é—´ï¼š{target_time}ï¼‰")
        
        # 2. é€ä¸ªè¯»å–å˜é‡
        result = {}
        for gfs_var in gfs_vars:
            if gfs_var not in self.gfs_var_type:
                raise ValueError(f"GFSä¸æ”¯æŒè¯¥å˜é‡ï¼š{gfs_var}ï¼Œæ”¯æŒçš„å˜é‡ï¼š{self.all_gfs_vars_list}")
            
            var_type = self.gfs_var_type[gfs_var]
            var_data = self.ds[gfs_var].isel(time=time_idx)  # å–æŒ‡å®šæ—¶é—´
            
            if var_type == "upper_air":
                # é«˜ç©ºå˜é‡ï¼šæŒ‰å›ºå®šå±‚çº§é¡ºåºé‡æ–°æ’åº â†’ (13, 721, 1440)
                level_vals = self.ds["level"].values.tolist()
                try:
                    level_indices = [level_vals.index(level) for level in self.level_order]
                except ValueError as e:
                    raise ValueError(f"GFSå±‚çº§ç¼ºå¤±ï¼š{e}ï¼Œå½“å‰å±‚çº§ï¼š{level_vals}")
                var_data_sorted = var_data.isel(level=level_indices)
                var_arr = var_data_sorted.values  # (13, 721, 1440)
            else:
                # åœ°é¢å˜é‡ï¼šå¢åŠ å±‚æ•°ç»´åº¦ â†’ (1, 721, 1440)
                var_arr = var_data.values[np.newaxis, :, :]
            
            # å¤„ç†ç¼ºå¤±å€¼
            var_arr = np.nan_to_num(var_arr, nan=0.0)
            result[gfs_var] = var_arr
            print(f"   GFSå˜é‡{gfs_var}ï¼šå½¢çŠ¶{var_arr.shape}ï¼ˆ{var_type}ï¼‰")
        
        return result

    @property
    def all_gfs_vars(self) -> List[str]:
        """è¿”å›æ‰€æœ‰æ”¯æŒçš„GFSå˜é‡å"""
        return self.all_gfs_vars_list

    def close(self):
        """å…³é—­Datasetå¥æŸ„ï¼Œé‡Šæ”¾èµ„æº"""
        if self.ds is not None:
            self.ds.close()
            print("âœ… GFS Datasetå·²å…³é—­")



import numpy as np
import torch
from torch.utils.data import Dataset

def pad_to_base_layers(data: np.ndarray, base_layers: int = 13, pad_mode: str = "repeat") -> np.ndarray:
    """
    data: (D, H, W), D in {1, base_layers}
    return: (base_layers, H, W)
    """
    D, H, W = data.shape
    if D == base_layers:
        return data
    if D != 1:
        raise ValueError(f"å˜é‡å±‚æ•°å¿…é¡»æ˜¯1æˆ–{base_layers}ï¼Œå½“å‰ä¸º{D}")

    if pad_mode == "repeat":
        return np.repeat(data, base_layers, axis=0)
    elif pad_mode == "zero":
        out = np.zeros((base_layers, H, W), dtype=data.dtype)
        out[0:1] = data
        return out
    else:
        raise ValueError("pad_mode must be 'repeat' or 'zero'")

class GFSERA5PairDataset(Dataset):
    """
    å•æ ·æœ¬è¿”å›:
      gfs: (L, V, H, W)
      era5:(L, V, H, W)
      ts:  str
    å…¶ä¸­ L=base_layers(é»˜è®¤13), V=len(gfs_vars)
    """
    def __init__(
        self,
        gfs_reader,
        era5_reader,
        gfs_vars=None,
        base_layers: int = 13,
        pad_mode: str = "repeat",
        normalize: bool = False,
        norm_sample_num: int = 100,
        eps: float = 1e-8,
    ):
        self.gfs_reader = gfs_reader
        self.era5_reader = era5_reader
        self.gfs_vars = gfs_vars or list(getattr(gfs_reader, "all_gfs_vars"))
        self.base_layers = base_layers
        self.pad_mode = pad_mode
        self.normalize = normalize
        self.norm_sample_num = norm_sample_num
        self.eps = eps

        # å–äº¤é›†æ—¶é—´æˆ³ï¼Œç¡®ä¿ä¸¥æ ¼é…å¯¹ï¼ˆä¸ä¼šâ€œå„è‡ª nearest é€ æˆé”™é…â€ï¼‰
        gfs_times = set(gfs_reader.time_index)
        era5_times = set(era5_reader.time_index)
        self.common_timestamps = sorted(list(gfs_times & era5_times))
        if len(self.common_timestamps) == 0:
            raise ValueError("GFS å’Œ ERA5 æ²¡æœ‰é‡å æ—¶é—´æˆ³ï¼Œæ— æ³•é…å¯¹")

        # (å¯é€‰) æŒ‰å˜é‡åšæ ‡å‡†åŒ–ï¼šæ¯ä¸ªå˜é‡ç‹¬ç«‹ mean/stdï¼ˆå¯¹ pad åçš„ (L,H,W) ç»Ÿè®¡ï¼‰
        self.norm_params = None
        if self.normalize:
            self.norm_params = self._compute_norm_params()

    def _compute_norm_params(self):
        params = {}
        n = min(self.norm_sample_num, len(self.common_timestamps))
        for var in self.gfs_vars:
            buf = []
            for ts in self.common_timestamps[:n]:
                g = self.gfs_reader.read_by_time(ts, [var])[var]
                e = self.era5_reader.read_by_time(ts, [var])[var]
                g = pad_to_base_layers(g, self.base_layers, self.pad_mode)
                e = pad_to_base_layers(e, self.base_layers, self.pad_mode)
                buf.append(g); buf.append(e)
            x = np.stack(buf, axis=0)  # (2n, L, H, W)
            mean = np.mean(x, axis=(0, 2, 3), keepdims=True)  # (1, L, 1, 1)
            std = np.std(x, axis=(0, 2, 3), keepdims=True) + self.eps
            params[var] = (mean, std)
        return params

    def _norm(self, x: np.ndarray, var: str) -> np.ndarray:
        mean, std = self.norm_params[var]
        # x: (L,H,W) -> broadcast with (1,L,1,1) by adding axis
        return (x[None, ...] - mean) / std  # (1,L,H,W)

    def __len__(self):
        return len(self.common_timestamps)

    def __getitem__(self, idx):
        ts = self.common_timestamps[idx]
        ts_str = ts.strftime("%Y-%m-%d %H:%M:%S")

        gfs_vars_LVHW = []
        era5_vars_LVHW = []

        for var in self.gfs_vars:
            g = self.gfs_reader.read_by_time(ts, [var])[var]   # (D,H,W)
            e = self.era5_reader.read_by_time(ts, [var])[var]  # (D,H,W)

            g = pad_to_base_layers(g, self.base_layers, self.pad_mode)  # (L,H,W)
            e = pad_to_base_layers(e, self.base_layers, self.pad_mode)  # (L,H,W)

            if self.normalize:
                g = self._norm(g, var)[0]  # back to (L,H,W)
                e = self._norm(e, var)[0]

            # å˜æˆ (L,1,H,W)
            gfs_vars_LVHW.append(torch.from_numpy(g).float().unsqueeze(1))
            era5_vars_LVHW.append(torch.from_numpy(e).float().unsqueeze(1))

        # åˆå¹¶å˜é‡ç»´ï¼š (L,V,H,W)
        gfs = torch.cat(gfs_vars_LVHW, dim=1)
        era5 = torch.cat(era5_vars_LVHW, dim=1)

        return gfs, era5, ts_str

def collate_fn(batch, base_layers: int = 13):
    """
    è¾“å…¥batch: List[(gfs(L,V,H,W), era5(L,V,H,W), ts_str)]
    è¾“å‡º:
      gfs_batch: (B*L, V, H, W)
      era5_batch: (B*L, V, H, W)
      ts_batch:   List[str] é•¿åº¦ B*Lï¼ˆæ¯å±‚å¤åˆ¶æ—¶é—´æˆ³ï¼‰
    """
    g_list, e_list, ts_list = [], [], []
    for g, e, ts in batch:
        if g.ndim != 4 or g.shape[0] != base_layers:
            raise ValueError(f"æœŸæœ›å•æ ·æœ¬ gfs ä¸º ({base_layers},V,H,W)ï¼Œå®é™… {tuple(g.shape)}")
        if e.ndim != 4 or e.shape[0] != base_layers:
            raise ValueError(f"æœŸæœ›å•æ ·æœ¬ era5 ä¸º ({base_layers},V,H,W)ï¼Œå®é™… {tuple(e.shape)}")
        g_list.append(g)
        e_list.append(e)
        ts_list.append(ts)

    g_stack = torch.stack(g_list, dim=0)  # (B,L,V,H,W)
    e_stack = torch.stack(e_list, dim=0)  # (B,L,V,H,W)

    B, L, V, H, W = g_stack.shape
    g_batch = g_stack.reshape(B * L, V, H, W)
    e_batch = e_stack.reshape(B * L, V, H, W)

    # æ¯ä¸ªæ—¶é—´æˆ³å¤åˆ¶ L æ¬¡ï¼Œå¯¹é½ B*L
    ts_out = []
    for ts in ts_list:
        ts_out.extend([ts] * L)

    return g_batch, e_batch, ts_out



if __name__ == "__main__":
    # 1. åˆå§‹åŒ–Reader
    gfs_reader = GFSReader(
        start_dt="2020-01-01 00:00:00",
        end_dt="2020-01-02 18:00:00"
    )
    era5_reader = ERA5Reader(
        start_dt="2020-01-01 00:00:00",
        end_dt="2020-01-02 18:00:00"
    )
    
    # 2. é€‰æ‹©è¦è®­ç»ƒçš„å˜é‡ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
    train_vars = [
        "Temperature",              # 13å±‚
        "10 metre U wind component" # 1å±‚
    ]
    
    # 3. åˆå§‹åŒ–æ•°æ®é›†
    dataset = GFSERA5PairDataset(
        gfs_reader=gfs_reader,
        era5_reader=era5_reader,
        gfs_vars=train_vars,
        normalize=False,
        base_layers=13,
        pad_mode="repeat"  # æ¨èç”¨repeatï¼ˆç‰©ç†æ„ä¹‰æ›´åˆç†ï¼‰
    )
    
    # 4. æµ‹è¯•å•æ ·æœ¬
    gfs_tensor, era5_tensor, ts_str = dataset[0]
    print(f"\n=== å•æ ·æœ¬ç»´åº¦ ===")
    print(f"æ—¶é—´æˆ³ï¼š{ts_str}")
    print(f"å•æ ·æœ¬å½¢çŠ¶ï¼ˆå±‚æ•°Ã—å˜é‡æ•°Ã—çº¬åº¦Ã—ç»åº¦ï¼‰ï¼š{gfs_tensor.shape}")
    print(f"  - å±‚æ•°ç»´åº¦ï¼š{gfs_tensor.shape[0]}ï¼ˆç»Ÿä¸€ä¸º13å±‚ï¼‰")
    print(f"  - å˜é‡æ•°ï¼ˆChannelsï¼‰ï¼š{gfs_tensor.shape[1]}ï¼ˆ{len(train_vars)}ä¸ªï¼‰")
    print(f"  - ç©ºé—´ç»´åº¦ï¼š{gfs_tensor.shape[2]}Ã—{gfs_tensor.shape[3]}")
    
    # 5. åˆå§‹åŒ–DataLoaderï¼ˆbatch_size=2ï¼‰
    batch_size = 2
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        # å¼ºåˆ¶æŒ‡å®šcollate_fnï¼Œé¿å…ä½¿ç”¨é»˜è®¤å‡½æ•°
        collate_fn=lambda x: collate_fn(x, base_layers=13)
    )
    
    # 6. æµ‹è¯•æ‰¹é‡æ•°æ®
    # æ›¿æ¢åŸæœ‰æµ‹è¯•å¾ªç¯
    for batch_idx, (gfs_batch, era5_batch, ts_batch) in enumerate(dataloader):
        print(f"[DataLoader] batch_idx={batch_idx}, gfs_batch={tuple(gfs_batch.shape)}, era5_batch={tuple(era5_batch.shape)}")
        print(f"[DataLoader] merged_batch={gfs_batch.shape[0]} (åº”è¯¥ç­‰äº batch_size*13)")
        break
    
    # 7. å…³é—­èµ„æº
    gfs_reader.close()
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")