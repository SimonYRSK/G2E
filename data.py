import zarr
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import torch
from torch.utils.data import Dataset, DataLoader
import xarray as xr
from tqdm.auto import tqdm
import time

gfs2era5_mapping = {
    "Temperature": {
        "era5_vars": ["t50", "t100", "t150", "t200", "t250", "t300", "t400", "t500", "t600", "t700", "t850", "t925", "t1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨æ°”æ¸© â†’ ERA5 50~1000hPaç­‰å‹é¢æ°”æ¸©ï¼ˆ13å±‚ï¼‰"
    },
    "2 metre temperature": {
        "era5_vars": ["t2m"],
        "var_type": "surface",
        "description": "GFS 2ç±³æ°”æ¸© â†’ ERA5 2ç±³æ°”æ¸©ï¼ˆt2mï¼‰"
    },
    "10 metre U wind component": {
        "era5_vars": ["u10m"],
        "var_type": "surface",
        "description": "GFS 10ç±³Ué£åˆ†é‡ â†’ ERA5 10ç±³çº¬å‘é£ï¼ˆu10mï¼‰"
    },
    "100 metre U wind component": {
        "era5_vars": ["u100m"],
        "var_type": "surface",
        "description": "GFS 100ç±³Ué£åˆ†é‡ â†’ ERA5 100ç±³çº¬å‘é£ï¼ˆu100mï¼‰"
    },
    "10 metre V wind component": {
        "era5_vars": ["v10m"],
        "var_type": "surface",
        "description": "GFS 10ç±³Vé£åˆ†é‡ â†’ ERA5 10ç±³ç»å‘é£ï¼ˆv10mï¼‰"
    },
    "100 metre V wind component": {
        "era5_vars": ["v100m"],
        "var_type": "surface",
        "description": "GFS 100ç±³Vé£åˆ†é‡ â†’ ERA5 100ç±³ç»å‘é£ï¼ˆv100mï¼‰"
    },
    "U component of wind": {
        "era5_vars": ["u50", "u100", "u150", "u200", "u250", "u300", "u400", "u500", "u600", "u700", "u850", "u925", "u1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨Ué£åˆ†é‡ â†’ ERA5 50~1000hPaç­‰å‹é¢çº¬å‘é£ï¼ˆ13å±‚ï¼‰"
    },
    "V component of wind": {
        "era5_vars": ["v50", "v100", "v150", "v200", "v250", "v300", "v400", "v500", "v600", "v700", "v850", "v925", "v1000"],
        "var_type": "upper_air",
        "description": "GFSé€šç”¨Vé£åˆ†é‡ â†’ ERA5 50~1000hPaç­‰å‹é¢ç»å‘é£ï¼ˆ13å±‚ï¼‰"
    },
    "Geopotential height": {
        "era5_vars": ["z50", "z100", "z150", "z200", "z250", "z300", "z400", "z500", "z600", "z700", "z850", "z925", "z1000"],
        "var_type": "upper_air",
        "description": "GFSä½åŠ¿é«˜åº¦ â†’ ERA5 50~1000hPaç­‰å‹é¢ä½åŠ¿é«˜åº¦ï¼ˆ13å±‚ï¼‰"
    },
    "2 metre dewpoint temperature": {
        "era5_vars": ["d2m"],
        "var_type": "surface",
        "description": "GFS 2ç±³éœ²ç‚¹æ¸©åº¦ â†’ ERA5 2ç±³éœ²ç‚¹æ¸©åº¦ï¼ˆd2mï¼‰"
    }
}

# åå‘æ˜ å°„ï¼šERA5å˜é‡å â†’ GFSé€šç”¨å˜é‡å
era52gfs_mapping = {}
for gfs_var, info in gfs2era5_mapping.items():
    for era5_var in info["era5_vars"]:
        era52gfs_mapping[era5_var] = gfs_var


class ERA5Reader:
    def __init__(self,
                zarr_path: str = "/cpfs01/projects-HDD/cfff-4a8d9af84f66_HDD/public/fanjiang/dataset/era5.2002_2024.c85.p25.h6",
                mapping: Dict = gfs2era5_mapping,
                start_dt: str = "2020-01-01 00:00:00",
                end_dt: str = "2024-12-31 18:00:00",
                reverse_mapping: Dict = era52gfs_mapping):
        
        # è·¯å¾„å¤„ç†ï¼ˆå…¼å®¹å­—ç¬¦ä¸²/Pathå¯¹è±¡ï¼‰
        self.era5_root = Path(zarr_path) if isinstance(zarr_path, str) else zarr_path
        self.mapping = mapping
        self.reverse_mapping = reverse_mapping
        
        # æ—¶é—´å‚æ•°åˆå§‹åŒ–
        self.start_dt = datetime.strptime(start_dt, "%Y-%m-%d %H:%M:%S")
        self.end_dt = datetime.strptime(end_dt, "%Y-%m-%d %H:%M:%S")
        self.base_time = datetime(2002, 1, 1, 0, 0, 0)  # ERA5æ•°æ®é›†åŸºå‡†æ—¶é—´
        self.time_step_hours = 6.0  # ERA5å›ºå®š6å°æ—¶æ—¶é—´æ­¥
        
        # é¢„ç¼“å­˜ï¼šGFSé€šç”¨å˜é‡å â†’ ERA5å˜é‡åˆ—è¡¨
        self.gfs2era5_vars = {k: v["era5_vars"] for k, v in mapping.items()}
        
        # æ‡’åŠ è½½Zarrå¥æŸ„å’Œæœ‰æ•ˆæ—¶é—´ç´¢å¼•
        self.data_zarr = None
        self.channel_zarr = None
        self.lat_zarr = None
        self.lon_zarr = None
        self.valid_time_indices = None  # ç­›é€‰åçš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•
        self.channel_name2idx = None    # channelåâ†’ç´¢å¼•æ˜ å°„
        self._load_zarr_handles()
        
        # é¢„ç”Ÿæˆæœ‰æ•ˆæ—¶é—´æˆ³åˆ—è¡¨ï¼ˆç´¢å¼•â†’å®é™…æ—¶é—´ï¼‰
        self.valid_timestamps = self._generate_valid_timestamps()

    def _load_zarr_handles(self):
        """æ‡’åŠ è½½Zarrå¥æŸ„ï¼ˆæ ¸å¿ƒï¼šé¿å…ä¸€æ¬¡æ€§åŠ è½½è¶…å¤§æ•°ç»„ï¼‰"""
        try:
            # åŠ è½½å„ç»´åº¦Zarrå¥æŸ„ï¼ˆåªè¯»æ¨¡å¼ï¼‰
            self.data_zarr = zarr.open(self.era5_root / "data", mode='r')
            self.channel_zarr = zarr.open(self.era5_root / "channel", mode='r')
            self.lat_zarr = zarr.open(self.era5_root / "lat", mode='r')
            self.lon_zarr = zarr.open(self.era5_root / "lon", mode='r')
            
            # åŸºç¡€ç»´åº¦ä¿¡æ¯
            self.time_steps = self.data_zarr.shape[0]  # timeç»´åº¦é•¿åº¦
            self.n_channels = self.data_zarr.shape[1]  # channelç»´åº¦é•¿åº¦
            self.lat_size = self.data_zarr.shape[2]    # çº¬åº¦æ•°
            self.lon_size = self.data_zarr.shape[3]    # ç»åº¦æ•°
            
            # æ ¡éªŒç©ºé—´ç»´åº¦ï¼ˆ721Ã—1440ï¼‰
            assert self.lat_size == 721 and self.lon_size == 1440, \
                f"ERA5ç©ºé—´ç»´åº¦é”™è¯¯ï¼šéœ€721Ã—1440ï¼Œå½“å‰{self.lat_size}Ã—{self.lon_size}"
            
            # å…¼å®¹å­—ç¬¦ä¸²/å­—èŠ‚ä¸²çš„channelå
            channel_raw = self.channel_zarr[:]
            self.channel_names = []
            for name in channel_raw:
                if isinstance(name, bytes):
                    self.channel_names.append(name.decode('utf-8'))
                elif isinstance(name, (str, np.str_)):
                    self.channel_names.append(str(name))
                else:
                    self.channel_names.append(str(name))
            
            # æ„å»ºchannelåâ†’ç´¢å¼•æ˜ å°„
            self.channel_name2idx = {name: idx for idx, name in enumerate(self.channel_names)}
            
            # ç­›é€‰æœ‰æ•ˆæ—¶é—´ç´¢å¼•
            self.valid_time_indices = self._time_filter()
            print(f"âœ… ERA5åŠ è½½å®Œæˆï¼š")
            print(f"   æœ‰æ•ˆæ—¶é—´æ­¥ï¼š{len(self.valid_time_indices)}ä¸ªï¼ˆ{self.start_dt}~{self.end_dt}ï¼‰")
            print(f"   æ€»é€šé“æ•°ï¼š{self.n_channels}ï¼Œç©ºé—´ç»´åº¦ï¼š{self.lat_size}Ã—{self.lon_size}")
            print(f"   å‰5ä¸ªé€šé“åï¼š{self.channel_names[:5]}")
        
        except Exception as e:
            raise RuntimeError(f"åŠ è½½ERA5 Zarrå¤±è´¥ï¼š{str(e)}")

    def _time_filter(self) -> List[int]:
        """ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•"""
        # è®¡ç®—èµ·å§‹/ç»“æŸæ—¶é—´å¯¹åº”çš„ç´¢å¼•
        delta_hours_start = (self.start_dt - self.base_time).total_seconds() / 3600
        start_idx = int(delta_hours_start // self.time_step_hours)
        
        delta_hours_end = (self.end_dt - self.base_time).total_seconds() / 3600
        end_idx = int(delta_hours_end // self.time_step_hours)
        
        # è¾¹ç•Œæ ¡éªŒ
        start_idx = max(0, start_idx)
        end_idx = min(self.time_steps - 1, end_idx)
        
        if start_idx > end_idx:
            raise ValueError(f"ERA5æ— æœ‰æ•ˆæ—¶é—´æ•°æ®ï¼š{self.start_dt}~{self.end_dt}ï¼ˆç´¢å¼•{start_idx}~{end_idx}è¶…å‡ºèŒƒå›´ï¼‰")
        
        return list(range(start_idx, end_idx + 1))

    def _generate_valid_timestamps(self) -> List[datetime]:
        """ç”Ÿæˆæœ‰æ•ˆæ—¶é—´ç´¢å¼•å¯¹åº”çš„å®é™…æ—¶é—´æˆ³åˆ—è¡¨"""
        valid_timestamps = []
        for time_idx in self.valid_time_indices:
            delta_hours = time_idx * self.time_step_hours
            timestamp = self.base_time + timedelta(hours=delta_hours)
            valid_timestamps.append(timestamp)
        return valid_timestamps

    def _get_nearest_time_idx(self, target_time: datetime) -> int:
        """æ ¹æ®ç›®æ ‡æ—¶é—´æ‰¾æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•"""
        delta_hours = (target_time - self.base_time).total_seconds() / 3600
        target_idx = int(delta_hours // self.time_step_hours)
        
        # æ‰¾æœ‰æ•ˆç´¢å¼•ä¸­æœ€æ¥è¿‘çš„
        valid_indices_arr = np.array(self.valid_time_indices)
        nearest_idx = valid_indices_arr[np.argmin(np.abs(valid_indices_arr - target_idx))]
        
        if nearest_idx not in self.valid_time_indices:
            raise ValueError(f"ç›®æ ‡æ—¶é—´{target_time}æ— å¯¹åº”çš„æœ‰æ•ˆERA5æ•°æ®")
        return nearest_idx

    def read_by_time(self, target_time: datetime, gfs_vars: Optional[List[str]] = None, verbose: bool = False) -> Dict[str, np.ndarray]:
        """
        æŒ‰ç›®æ ‡æ—¶é—´è¯»å–ERA5æ•°æ®ï¼Œè¿”å›{GFSé€šç”¨å˜é‡å: (å±‚æ•°, lat, lon)}
        """
        gfs_vars = gfs_vars or list(self.gfs2era5_vars.keys())
        
        # 1. æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•
        time_idx = self._get_nearest_time_idx(target_time)
        
        # 2. é€ä¸ªè¯»å–GFSé€šç”¨å˜é‡å¯¹åº”çš„ERA5åˆ†å±‚å˜é‡
        result = {}
        for gfs_var in gfs_vars:
            if gfs_var not in self.gfs2era5_vars:
                raise ValueError(f"ERA5ä¸æ”¯æŒGFSé€šç”¨å˜é‡ï¼š{gfs_var}")
            
            era5_vars = self.gfs2era5_vars[gfs_var]
            layer_data_list = []
            
            for era5_var in era5_vars:
                if era5_var not in self.channel_name2idx:
                    raise ValueError(f"ERA5æ— è¯¥é€šé“ï¼š{era5_var}")
                chan_idx = self.channel_name2idx[era5_var]
                var_data = self.data_zarr[time_idx, chan_idx, :, :]  # (lat, lon)
                layer_data_list.append(var_data)
            
            var_data_3d = np.stack(layer_data_list, axis=0)
            result[gfs_var] = var_data_3d

            if verbose:
                print(f"ğŸ“Œ ERA5é€šç”¨å˜é‡{gfs_var}ï¼šæ‹¼æ¥{len(era5_vars)}å±‚ï¼Œå½¢çŠ¶{var_data_3d.shape}")
        
        return result

    @property
    def time_index(self) -> pd.DatetimeIndex:
        """è¿”å›æœ‰æ•ˆæ—¶é—´æˆ³çš„DatetimeIndexï¼ˆå’ŒGFSReaderå¯¹é½ï¼‰"""
        return pd.DatetimeIndex(self.valid_timestamps)

    @property
    def all_gfs_vars(self) -> List[str]:
        """è¿”å›æ‰€æœ‰æ”¯æŒçš„GFSé€šç”¨å˜é‡å"""
        return list(self.gfs2era5_vars.keys())


class GFSReader:
    def __init__(self,
                zarr_path: str = "/cpfs01/projects-HDD/cfff-4a8d9af84f66_HDD/public/database/gfs_2020_2024_c10",
                mapping: Dict = gfs2era5_mapping,
                start_dt: str = "2020-01-01 00:00:00",
                end_dt: str = "2024-12-31 18:00:00",
                reverse_mapping: Dict = era52gfs_mapping):
        
        # è·¯å¾„å¤„ç†
        self.gfs_root = Path(zarr_path) if isinstance(zarr_path, str) else zarr_path
        self.mapping = mapping
        self.reverse_mapping = reverse_mapping
        
        # æ—¶é—´å‚æ•°åˆå§‹åŒ–
        self.start_dt = datetime.strptime(start_dt, "%Y-%m-%d %H:%M:%S")
        self.end_dt = datetime.strptime(end_dt, "%Y-%m-%d %H:%M:%S")
        
        # é¢„ç¼“å­˜ï¼šGFSé€šç”¨å˜é‡å â†’ å˜é‡ç±»å‹ï¼ˆé«˜ç©º/åœ°é¢ï¼‰
        self.gfs_var_type = {k: v["var_type"] for k, v in mapping.items()}
        # é¢„ç¼“å­˜ï¼šæ‰€æœ‰æ”¯æŒçš„GFSå˜é‡å
        self.all_gfs_vars_list = list(mapping.keys())
        
        # æ‡’åŠ è½½å±æ€§
        self.ds = None  # xarray Datasetå¥æŸ„
        self.time_index = None  # æœ‰æ•ˆæ—¶é—´ç´¢å¼•ï¼ˆDatetimeIndexï¼‰
        self.valid_time_indices = None  # ç­›é€‰åçš„åŸå§‹æ—¶é—´ç´¢å¼•ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰
        self.valid_time_stamps = None   # ç­›é€‰åçš„æ—¶é—´æˆ³åˆ—è¡¨
        self.level_order = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]  # å›ºå®šå±‚çº§é¡ºåº
        
        # åˆå§‹åŒ–åŠ è½½
        self._load_zarr_handles()
        self._filter_valid_times()

    def _load_zarr_handles(self):
        """æ‡’åŠ è½½GFS Zarræ•°æ®ï¼ˆxarrayæ–¹å¼ï¼‰"""
        try:
            self.ds = xr.open_zarr(self.gfs_root)
            print(f"âœ… GFS ZarråŠ è½½å®Œæˆï¼šè·¯å¾„={self.gfs_root}")
            
            # éªŒè¯æ ¸å¿ƒç»´åº¦
            required_dims = ["time", "lat", "lon", "level"]
            missing_dims = [d for d in required_dims if d not in self.ds.dims]
            if missing_dims:
                raise ValueError(f"GFS Zarrç¼ºå¤±æ ¸å¿ƒç»´åº¦ï¼š{missing_dims}")
            
            # æ„å»ºå®Œæ•´çš„æ—¶é—´ç´¢å¼•
            self.full_time_index = pd.DatetimeIndex(self.ds["time"].values.astype('datetime64[s]'))
            print(f"   GFSæ€»æ—¶é—´èŒƒå›´ï¼š{self.full_time_index[0]} ~ {self.full_time_index[-1]}")
            print(f"   GFSæ€»æ—¶é—´æ­¥ï¼š{len(self.full_time_index)}")
            print(f"   GFSç©ºé—´ç»´åº¦ï¼šlat={self.ds.dims['lat']}, lon={self.ds.dims['lon']}")
        
        except Exception as e:
            raise RuntimeError(f"åŠ è½½GFS Zarrå¤±è´¥ï¼š{str(e)}")

    def _filter_valid_times(self):
        """ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œæ—¶é—´æˆ³"""
        mask = (self.full_time_index >= self.start_dt) & (self.full_time_index <= self.end_dt)
        self.valid_time_indices = np.where(mask)[0].tolist()
        self.valid_time_stamps = self.full_time_index[mask].tolist()
        
        if not self.valid_time_indices:
            raise ValueError(f"GFSæ— æœ‰æ•ˆæ—¶é—´æ•°æ®ï¼š{self.start_dt}~{self.end_dt}")
        
        self.time_index = pd.DatetimeIndex(self.valid_time_stamps)
        print(f"âœ… GFSæœ‰æ•ˆæ—¶é—´æ­¥ï¼š{len(self.valid_time_indices)}ä¸ªï¼ˆ{self.start_dt}~{self.end_dt}ï¼‰")

    def _get_nearest_time_idx(self, target_time: datetime) -> Tuple[int, datetime]:
        """æ‰¾åˆ°ç›®æ ‡æ—¶é—´æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œå¯¹åº”çš„æ—¶é—´æˆ³"""
        time_diffs = [abs((ts - target_time).total_seconds()) for ts in self.valid_time_stamps]
        nearest_pos = np.argmin(time_diffs)
        nearest_raw_idx = self.valid_time_indices[nearest_pos]
        nearest_ts = self.valid_time_stamps[nearest_pos]
        return nearest_raw_idx, nearest_ts

    def read_by_time(self, target_time: datetime, gfs_vars: Optional[List[str]] = None) -> Dict[str, np.ndarray]:
        """æŒ‰ç›®æ ‡æ—¶é—´è¯»å–GFSæ•°æ®ï¼ˆå’ŒERA5Readeræ¥å£å®Œå…¨å¯¹é½ï¼‰"""
        gfs_vars = gfs_vars or self.all_gfs_vars_list
        
        # 1. æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆæ—¶é—´ç´¢å¼•å’Œæ—¶é—´æˆ³
        time_idx, target_time_str = self._get_nearest_time_idx(target_time)
        print(f"ğŸ“Œ è¯»å–GFSæ—¶é—´ï¼š{target_time_str}ï¼ˆç›®æ ‡æ—¶é—´ï¼š{target_time}ï¼‰")
        
        # 2. é€ä¸ªè¯»å–å˜é‡
        result = {}
        for gfs_var in gfs_vars:
            if gfs_var not in self.gfs_var_type:
                raise ValueError(f"GFSä¸æ”¯æŒè¯¥å˜é‡ï¼š{gfs_var}ï¼Œæ”¯æŒçš„å˜é‡ï¼š{self.all_gfs_vars_list}")
            
            var_type = self.gfs_var_type[gfs_var]
            var_data = self.ds[gfs_var].isel(time=time_idx)  # å–æŒ‡å®šæ—¶é—´
            
            if var_type == "upper_air":
                # é«˜ç©ºå˜é‡ï¼šæŒ‰å›ºå®šå±‚çº§é¡ºåºé‡æ–°æ’åº â†’ (13, 721, 1440)
                level_vals = self.ds["level"].values.tolist()
                try:
                    level_indices = [level_vals.index(level) for level in self.level_order]
                except ValueError as e:
                    raise ValueError(f"GFSå±‚çº§ç¼ºå¤±ï¼š{e}ï¼Œå½“å‰å±‚çº§ï¼š{level_vals}")
                var_data_sorted = var_data.isel(level=level_indices)
                var_arr = var_data_sorted.values  # (13, 721, 1440)
            else:
                # åœ°é¢å˜é‡ï¼šå¢åŠ å±‚æ•°ç»´åº¦ â†’ (1, 721, 1440)
                var_arr = var_data.values[np.newaxis, :, :]
            
            # å¤„ç†ç¼ºå¤±å€¼
            var_arr = np.nan_to_num(var_arr, nan=0.0)
            result[gfs_var] = var_arr
            print(f"   GFSå˜é‡{gfs_var}ï¼šå½¢çŠ¶{var_arr.shape}ï¼ˆ{var_type}ï¼‰")
        
        return result

    @property
    def all_gfs_vars(self) -> List[str]:
        """è¿”å›æ‰€æœ‰æ”¯æŒçš„GFSå˜é‡å"""
        return self.all_gfs_vars_list

    def close(self):
        """å…³é—­Datasetå¥æŸ„ï¼Œé‡Šæ”¾èµ„æº"""
        if self.ds is not None:
            self.ds.close()
            print("âœ… GFS Datasetå·²å…³é—­")



import numpy as np
import torch
from torch.utils.data import Dataset

def pad_to_base_layers(data: np.ndarray, base_layers: int = 13, pad_mode: str = "repeat") -> np.ndarray:
    """
    data: (D, H, W), D in {1, base_layers}
    return: (base_layers, H, W)
    """
    D, H, W = data.shape
    if D == base_layers:
        return data
    if D != 1:
        raise ValueError(f"å˜é‡å±‚æ•°å¿…é¡»æ˜¯1æˆ–{base_layers}ï¼Œå½“å‰ä¸º{D}")

    if pad_mode == "repeat":
        return np.repeat(data, base_layers, axis=0)
    elif pad_mode == "zero":
        out = np.zeros((base_layers, H, W), dtype=data.dtype)
        out[0:1] = data
        return out
    else:
        raise ValueError("pad_mode must be 'repeat' or 'zero'")

def _default_norm_cache_path(era5_reader, gfs_vars: List[str], base_layers: int, pad_mode: str) -> Path:
    # æ–‡ä»¶åé‡Œå¸¦ä¸Šæ—¶é—´èŒƒå›´ + L + pad_mode + å˜é‡æ•°é‡ï¼Œé¿å…æ··ç”¨
    start_str = era5_reader.start_dt.strftime("%Y%m%d%H")
    end_str = era5_reader.end_dt.strftime("%Y%m%d%H")
    fname = f"era5_norm_{start_str}_{end_str}_L{base_layers}_{pad_mode}_V{len(gfs_vars)}.npz"
    return Path(__file__).resolve().parent / fname


def _save_norm_npz(path: Path, params: Dict[str, Tuple[np.ndarray, np.ndarray]], meta: Dict[str, str]):
    arrays = {}
    arrays["__vars__"] = np.array(list(params.keys()), dtype=object)
    for k, v in meta.items():
        arrays[f"__meta__{k}"] = np.array(str(v), dtype=object)

    for var, (mean_L, std_L) in params.items():
        arrays[f"{var}__mean"] = mean_L.astype(np.float32)  # (L,)
        arrays[f"{var}__std"] = std_L.astype(np.float32)    # (L,)

    path.parent.mkdir(parents=True, exist_ok=True)
    np.savez_compressed(path, **arrays)

def _load_norm_npz(path: Path) -> Tuple[Dict[str, Tuple[np.ndarray, np.ndarray]], Dict[str, str]]:
    z = np.load(path, allow_pickle=True)
    vars_list = [str(x) for x in z["__vars__"].tolist()]
    meta = {}
    for key in z.files:
        if key.startswith("__meta__"):
            meta[key.replace("__meta__", "", 1)] = str(z[key].item())

    params = {}
    for var in vars_list:
        mean_L = z[f"{var}__mean"].astype(np.float32)  # (L,)
        std_L = z[f"{var}__std"].astype(np.float32)    # (L,)
        params[var] = (mean_L, std_L)
    return params, meta




class GFSERA5PairDataset(Dataset):
    """
    å•æ ·æœ¬è¿”å›:
      gfs: (L, V, H, W)
      era5:(L, V, H, W)
      ts:  str

    normalize=True æ—¶ï¼š
      - ä»…ç”¨ ERA5 åœ¨æ•´ä¸ªæ—¶é—´æ®µ(era5_reader.start_dt~end_dt)ç»Ÿè®¡ mean/std
      - æŒ‰ â€œå˜é‡ Ã— å±‚â€ ç»Ÿè®¡ï¼šmean/std å½¢çŠ¶ä¸º (L,)
      - åŒä¸€å¥—å‚æ•°åŒæ—¶ç”¨äº GFS å’Œ ERA5
      - ç¼“å­˜åˆ° npzï¼Œé¿å…æ¯æ¬¡é‡å¤ç»Ÿè®¡
    """
    def __init__(
        self,
        gfs_reader,
        era5_reader,
        gfs_vars=None,
        base_layers: int = 13,
        pad_mode: str = "repeat",
        normalize: bool = False,
        norm_cache_path: Optional[str] = None,
        eps: float = 1e-8,
    ):
        self.gfs_reader = gfs_reader
        self.era5_reader = era5_reader
        self.gfs_vars = gfs_vars or list(getattr(gfs_reader, "all_gfs_vars"))
        self.base_layers = base_layers
        self.pad_mode = pad_mode
        self.normalize = normalize
        self.eps = eps

        # å–äº¤é›†æ—¶é—´æˆ³ï¼Œç¡®ä¿ä¸¥æ ¼é…å¯¹
        gfs_times = set(gfs_reader.time_index)
        era5_times = set(era5_reader.time_index)
        self.common_timestamps = sorted(list(gfs_times & era5_times))
        if len(self.common_timestamps) == 0:
            raise ValueError("GFS å’Œ ERA5 æ²¡æœ‰é‡å æ—¶é—´æˆ³ï¼Œæ— æ³•é…å¯¹")

        self.norm_params: Optional[Dict[str, Tuple[np.ndarray, np.ndarray]]] = None
        if self.normalize:
            cache_path = Path(norm_cache_path) if norm_cache_path else _default_norm_cache_path(
                era5_reader=self.era5_reader,
                gfs_vars=self.gfs_vars,
                base_layers=self.base_layers,
                pad_mode=self.pad_mode,
            )

            if cache_path.exists():
                self.norm_params, _ = _load_norm_npz(cache_path)
                print(f"âœ… è¯»å–æ ‡å‡†åŒ–ç¼“å­˜ï¼š{cache_path}")
            else:
                self.norm_params = self._compute_era5_norm_params_over_full_period()
                meta = {
                    "start_dt": self.era5_reader.start_dt.strftime("%Y-%m-%d %H:%M:%S"),
                    "end_dt": self.era5_reader.end_dt.strftime("%Y-%m-%d %H:%M:%S"),
                    "base_layers": str(self.base_layers),
                    "pad_mode": str(self.pad_mode),
                }
                _save_norm_npz(cache_path, self.norm_params, meta)
                print(f"âœ… å·²ä¿å­˜æ ‡å‡†åŒ–ç¼“å­˜ï¼š{cache_path}")

    def _compute_era5_norm_params_over_full_period(self, time_block: int = 8):
        """
        æ›´å¿«ç‰ˆæœ¬ï¼šç›´æ¥ä» era5_reader.data_zarr åˆ†å—è¯» (time_block,H,W)ï¼Œç´¯è®¡ sum/sumsq
        - upper_air: 13ä¸ªé€šé“åˆ†åˆ«ç»Ÿè®¡ -> mean/std shape (13,)
        - surface: 1ä¸ªé€šé“ç»Ÿè®¡ï¼›pad_mode=repeat æ—¶å¤åˆ¶åˆ° 13 å±‚
        """
        params = {}
        print("normalizing...")
        z = self.era5_reader.data_zarr
        H = self.era5_reader.lat_size
        W = self.era5_reader.lon_size

        # ERA5Reader.valid_time_indices æ˜¯è¿ç»­ range(start,end)ï¼Œæ‰€ä»¥å¯ä»¥ç”¨ slice æ‰¹é‡è¯»
        t_start = self.era5_reader.valid_time_indices[0]
        t_end = self.era5_reader.valid_time_indices[-1] + 1
        nT = t_end - t_start

        # å»ºè®®è®© time_block å¯¹é½ zarr çš„ time chunk
        # æ¯”å¦‚ï¼štime_block = z.chunks[0] æˆ–è€…å®ƒçš„å€æ•°ï¼ˆå†…å­˜å…è®¸çš„è¯ï¼‰
        # print("zarr chunks:", getattr(z, "chunks", None))

        
        # å»ºè®®ä½ æŠŠ time_block è°ƒå¤§ä¸€ç‚¹ï¼š16/32/64ï¼ˆçœ‹å†…å­˜ï¼‰
        time_block = 64         # å…ˆè¯• 32ï¼Œé€šå¸¸æ¯” 8 å¿«
        chan_block = 4          # upper_air ä¸€æ¬¡è¯» 2 ä¸ªé€šé“ï¼›å¯è¯• 4ï¼ˆæ›´å¿«ä½†æ›´åƒå†…å­˜ï¼‰

        n_blocks = (nT + time_block - 1) // time_block

        for var in tqdm(self.gfs_vars, desc="ERA5 norm vars"):
            era5_vars = self.era5_reader.gfs2era5_vars[var]
            chan_indices = [self.era5_reader.channel_name2idx[v] for v in era5_vars]

            sum_L = np.zeros((self.base_layers,), dtype=np.float64)
            sumsq_L = np.zeros((self.base_layers,), dtype=np.float64)

            # surface: 1 ä¸ªé€šé“ï¼›upper_air: 13 ä¸ªé€šé“
            if len(chan_indices) == 1:
                chan_groups = [chan_indices]
                layer_groups = [np.array([0], dtype=int)]
            else:
                chan_groups = [chan_indices[i:i + chan_block] for i in range(0, len(chan_indices), chan_block)]
                layer_groups = [np.arange(i, i + len(g), dtype=int) for i, g in zip(range(0, len(chan_indices), chan_block), chan_groups)]

            pbar = tqdm(total=n_blocks * len(chan_groups), desc=f"{var} chunks", leave=False)

            for g_chans, g_layers in zip(chan_groups, layer_groups):
                for bi in range(n_blocks):
                    b0 = t_start + bi * time_block
                    b1 = min(b0 + time_block, t_end)

                    # å…³é”®ï¼šä¸€æ¬¡è¯»å¤šä¸ª channel
                    arr = z[b0:b1, g_chans, :, :]  # (Bt, Cg, H, W)
                    arr = np.asarray(arr)          # ç¡®ä¿ numpy array
                    np.nan_to_num(arr, nan=0.0, copy=False)

                    # ç›´æ¥æŒ‰è½´æ±‚å’Œï¼šå¯¹ time+H+W èšåˆï¼Œä¿ç•™ channel ç»´
                    s = arr.sum(axis=(0, 2, 3), dtype=np.float64)                 # (Cg,)
                    ss = (arr * arr).sum(axis=(0, 2, 3), dtype=np.float64)        # (Cg,)

                    sum_L[g_layers] += s
                    sumsq_L[g_layers] += ss

                    pbar.update(1)

            pbar.close()

            total_count = nT * H * W
            mean_L = sum_L / total_count
            var_L = sumsq_L / total_count - mean_L * mean_L
            var_L = np.maximum(var_L, 0.0)
            std_L = np.sqrt(var_L) + self.eps

            # surface pad
            if len(chan_indices) == 1:
                if self.pad_mode == "repeat":
                    mean_L = np.repeat(mean_L[0], self.base_layers)
                    std_L = np.repeat(std_L[0], self.base_layers)
                elif self.pad_mode == "zero":
                    mean_L[1:] = 0.0
                    std_L[1:] = self.eps
                else:
                    raise ValueError("pad_mode must be 'repeat' or 'zero'")

            params[var] = (mean_L.astype(np.float32), std_L.astype(np.float32))

        return params

    def _norm(self, x_LHW: np.ndarray, var: str) -> np.ndarray:
        """
        x_LHW: (L,H,W)
        ä½¿ç”¨ ERA5 ç»Ÿè®¡å¾—åˆ°çš„ mean/std: (L,)
        è¿”å›: (L,H,W)
        """
        mean_L, std_L = self.norm_params[var]
        mean = mean_L[:, None, None]
        std = std_L[:, None, None]
        return (x_LHW - mean) / std

    def __len__(self):
        return len(self.common_timestamps)

    def __getitem__(self, idx):
        ts = self.common_timestamps[idx]
        ts_str = ts.strftime("%Y-%m-%d %H:%M:%S")

        gfs_vars_LVHW = []
        era5_vars_LVHW = []

        for var in self.gfs_vars:
            g = self.gfs_reader.read_by_time(ts, [var])[var]         # (D,H,W)
            e = self.era5_reader.read_by_time(ts, [var], False)[var] # (D,H,W)

            g = pad_to_base_layers(g, self.base_layers, self.pad_mode)  # (L,H,W)
            e = pad_to_base_layers(e, self.base_layers, self.pad_mode)  # (L,H,W)

            if self.normalize:
                g = self._norm(g, var)
                e = self._norm(e, var)

            gfs_vars_LVHW.append(torch.from_numpy(g).float().unsqueeze(1))   # (L,1,H,W)
            era5_vars_LVHW.append(torch.from_numpy(e).float().unsqueeze(1))  # (L,1,H,W)

        gfs = torch.cat(gfs_vars_LVHW, dim=1)   # (L,V,H,W)
        era5 = torch.cat(era5_vars_LVHW, dim=1) # (L,V,H,W)

        return gfs, era5, ts_str


def collate_fn(batch, base_layers: int = 13):
    """
    è¾“å…¥batch: List[(gfs(L,V,H,W), era5(L,V,H,W), ts_str)]
    è¾“å‡º:
      gfs_batch: (B*L, V, H, W)
      era5_batch: (B*L, V, H, W)
      ts_batch:   List[str] é•¿åº¦ B*Lï¼ˆæ¯å±‚å¤åˆ¶æ—¶é—´æˆ³ï¼‰
    """
    g_list, e_list, ts_list = [], [], []
    for g, e, ts in batch:
        if g.ndim != 4 or g.shape[0] != base_layers:
            raise ValueError(f"æœŸæœ›å•æ ·æœ¬ gfs ä¸º ({base_layers},V,H,W)ï¼Œå®é™… {tuple(g.shape)}")
        if e.ndim != 4 or e.shape[0] != base_layers:
            raise ValueError(f"æœŸæœ›å•æ ·æœ¬ era5 ä¸º ({base_layers},V,H,W)ï¼Œå®é™… {tuple(e.shape)}")
        g_list.append(g)
        e_list.append(e)
        ts_list.append(ts)

    g_stack = torch.stack(g_list, dim=0)  # (B,L,V,H,W)
    e_stack = torch.stack(e_list, dim=0)  # (B,L,V,H,W)

    B, L, V, H, W = g_stack.shape
    g_batch = g_stack.reshape(B * L, V, H, W)
    e_batch = e_stack.reshape(B * L, V, H, W)

    # æ¯ä¸ªæ—¶é—´æˆ³å¤åˆ¶ L æ¬¡ï¼Œå¯¹é½ B*L
    ts_out = []
    for ts in ts_list:
        ts_out.extend([ts] * L)

    return g_batch, e_batch, ts_out



if __name__ == "__main__":
    # 1. åˆå§‹åŒ–Reader
    gfs_reader = GFSReader(
        start_dt="2020-01-01 00:00:00",
        end_dt="2024-12-31 18:00:00"
    )
    era5_reader = ERA5Reader(
        start_dt="2020-01-01 00:00:00",
        end_dt="2024-12-31 18:00:00"
    )
    
    # 2. é€‰æ‹©è¦è®­ç»ƒçš„å˜é‡ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
    train_vars = [
        "Temperature",
        "2 metre temperature",
        "10 metre U wind component",
        "100 metre U wind component",
        "10 metre V wind component",
        "100 metre V wind component",
        "U component of wind",
        "V component of wind",
        "Geopotential height",
        "2 metre dewpoint temperature"
    ]
    
    # 3. åˆå§‹åŒ–æ•°æ®é›†
    dataset = GFSERA5PairDataset(
        gfs_reader=gfs_reader,
        era5_reader=era5_reader,
        gfs_vars=train_vars,
        normalize=True,
        norm_cache_path="/cpfs01/projects-HDD/cfff-4a8d9af84f66_HDD/public/database/gfs_2020_2024_c10/era5_norm_1_8.npz",
        base_layers=13,
        pad_mode="repeat",
    )
    
    # 4. æµ‹è¯•å•æ ·æœ¬
    gfs_tensor, era5_tensor, ts_str = dataset[0]
    print(f"\n=== å•æ ·æœ¬ç»´åº¦ ===")
    print(f"æ—¶é—´æˆ³ï¼š{ts_str}")
    print(f"å•æ ·æœ¬å½¢çŠ¶ï¼ˆå±‚æ•°Ã—å˜é‡æ•°Ã—çº¬åº¦Ã—ç»åº¦ï¼‰ï¼š{gfs_tensor.shape}")
    print(f"  - å±‚æ•°ç»´åº¦ï¼š{gfs_tensor.shape[0]}ï¼ˆç»Ÿä¸€ä¸º13å±‚ï¼‰")
    print(f"  - å˜é‡æ•°ï¼ˆChannelsï¼‰ï¼š{gfs_tensor.shape[1]}ï¼ˆ{len(train_vars)}ä¸ªï¼‰")
    print(f"  - ç©ºé—´ç»´åº¦ï¼š{gfs_tensor.shape[2]}Ã—{gfs_tensor.shape[3]}")
    
    # 5. åˆå§‹åŒ–DataLoaderï¼ˆbatch_size=2ï¼‰
    batch_size = 2
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        # å¼ºåˆ¶æŒ‡å®šcollate_fnï¼Œé¿å…ä½¿ç”¨é»˜è®¤å‡½æ•°
        collate_fn=lambda x: collate_fn(x, base_layers=13)
    )
    
    # 6. æµ‹è¯•æ‰¹é‡æ•°æ®
    # æ›¿æ¢åŸæœ‰æµ‹è¯•å¾ªç¯
    for batch_idx, (gfs_batch, era5_batch, ts_batch) in enumerate(dataloader):
        print(f"[DataLoader] batch_idx={batch_idx}, gfs_batch={tuple(gfs_batch.shape)}, era5_batch={tuple(era5_batch.shape)}")
        print(f"[DataLoader] merged_batch={gfs_batch.shape[0]} (åº”è¯¥ç­‰äº batch_size*13)")
        break
    
    # 7. å…³é—­èµ„æº
    gfs_reader.close()
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")